{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a388486-0695-4707-9a2f-1a0be49c13ce",
   "metadata": {},
   "source": [
    "# üóÑÔ∏è Notebook 03 ‚Äî PostgreSQL Loading & Relational Validation\n",
    "\n",
    "## üéØ Objective of This Notebook\n",
    "\n",
    "Notebook 03 marks a **major architectural transition** in the project.\n",
    "\n",
    "Up to Notebook 02, all processing was performed in a **file-based analytical context** using standardized Parquet files. While that stage guaranteed **column-level correctness**, **unit consistency**, and **schema stability**, it deliberately avoided relational enforcement.\n",
    "\n",
    "This notebook exists to answer a different question:\n",
    "\n",
    "> ‚ÄúCan our standardized race data survive strict relational rules, repeated execution, and real analytical joins ‚Äî without breaking?‚Äù\n",
    "\n",
    "Notebook 03 is where the data pipeline becomes **structurally real**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Context from Notebook 02\n",
    "\n",
    "From Notebook 02, we now have:\n",
    "\n",
    "‚úÖ Fully standardized Parquet data  \n",
    "‚úÖ One directory per race  \n",
    "‚úÖ One canonical schema for each dataset  \n",
    "‚úÖ No ambiguity in column names or units  \n",
    "\n",
    "Directory structure:\n",
    "```\n",
    "data/interim/standardized/\n",
    "‚îî‚îÄ‚îÄ year=YYYY/\n",
    "    ‚îî‚îÄ‚îÄ round=XX_<race_name>/\n",
    "        ‚îú‚îÄ‚îÄ laps.parquet\n",
    "        ‚îú‚îÄ‚îÄ results.parquet\n",
    "        ‚îî‚îÄ‚îÄ track_status.parquet\n",
    "```\n",
    "Important guarantees from Notebook 02:\n",
    "\n",
    "- üü¢ Lap times are in **milliseconds**\n",
    "- üü¢ Lap grain is **one row per driver per lap**\n",
    "- üü¢ Driver identifiers are stable *within a race*\n",
    "- üü¢ No downstream notebook is allowed to ‚Äúfix‚Äù schema issues\n",
    "\n",
    "Notebook 03 **must adapt to this data**, not reshape it.\n",
    "\n",
    "---\n",
    "\n",
    "## üêò Why PostgreSQL Is Introduced Here\n",
    "\n",
    "PostgreSQL is introduced **only after standardization**, not before, because:\n",
    "\n",
    "- üì¶ Parquet cannot enforce:\n",
    "  - Primary keys\n",
    "  - Foreign keys\n",
    "  - Idempotent writes\n",
    "- üîó Strategy analysis requires **trustworthy joins**\n",
    "- üîÅ Re-runnable pipelines require **conflict handling**\n",
    "\n",
    "PostgreSQL in this project is:\n",
    "\n",
    "‚úÖ An analytical backbone  \n",
    "‚ùå Not a transformation engine  \n",
    "‚ùå Not a cleaning layer  \n",
    "\n",
    "---\n",
    "\n",
    "## üß± What This Notebook Will Do\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "### 1Ô∏è‚É£ Establish a PostgreSQL connection\n",
    "- Using environment-based configuration\n",
    "- Fully portable across machines\n",
    "\n",
    "### 2Ô∏è‚É£ Define strict relational schema\n",
    "Including:\n",
    "\n",
    "- **races**\n",
    "  - One row per race\n",
    "  - Identified by `race_id`\n",
    "- **drivers**\n",
    "  - One row per `(race_id, driver_code)`\n",
    "- **laps**\n",
    "  - One row per `(race_id, driver_code, lap_number)`\n",
    "\n",
    "### 3Ô∏è‚É£ Enforce explicit data grain\n",
    "- No implicit joins\n",
    "- No inferred keys\n",
    "- No tolerance for duplicates\n",
    "\n",
    "### 4Ô∏è‚É£ Load standardized Parquet ‚Üí PostgreSQL\n",
    "- One race at a time\n",
    "- With defensive column selection\n",
    "- Ignoring irrelevant columns safely\n",
    "\n",
    "### 5Ô∏è‚É£ Guarantee idempotency üîÅ\n",
    "- Safe kernel restarts\n",
    "- Safe re-execution\n",
    "- No duplicate inserts\n",
    "- No manual truncation required\n",
    "\n",
    "### 6Ô∏è‚É£ Validate relational integrity üß™\n",
    "- Row counts\n",
    "- Orphan detection\n",
    "- Lap grain uniqueness\n",
    "\n",
    "---\n",
    "\n",
    "## üö´ What This Notebook Will NOT Do\n",
    "\n",
    "This notebook explicitly does **not**:\n",
    "\n",
    "‚ùå Compute cumulative lap times  \n",
    "‚ùå Identify pit laps or out laps  \n",
    "‚ùå Detect undercuts  \n",
    "‚ùå Perform any race strategy logic  \n",
    "‚ùå Load track status into PostgreSQL  \n",
    "\n",
    "Those steps require **derived temporal context** and belong downstream.\n",
    "\n",
    "---\n",
    "\n",
    "## üö¶ Track Status: Intentional Deferral\n",
    "\n",
    "Although `track_status.parquet` exists for every race:\n",
    "\n",
    "- It is **event-based**, not lap-based\n",
    "- It cannot be joined meaningfully without lap timelines\n",
    "- Loading it prematurely caused grain violations during development\n",
    "\n",
    "‚û°Ô∏è Therefore:\n",
    "- Track status files are **validated for presence**\n",
    "- Actual usage is deferred to a later notebook\n",
    "\n",
    "This is a **design decision**, not a limitation.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Expected Outcome of Notebook 03\n",
    "\n",
    "By the end of this notebook, we expect:\n",
    "\n",
    "‚úî PostgreSQL populated with all races (2022‚Äì2024)  \n",
    "‚úî Strict relational invariants enforced  \n",
    "‚úî Fully idempotent execution  \n",
    "‚úî Database safe for analytical feature computation  \n",
    "\n",
    "Notebook 03 does **not produce insights** ‚Äî it produces **trust**.\n",
    "\n",
    "Everything that follows depends on this foundation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41479b49-e0f4-4378-881a-d20b6b82f848",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:27:11,925 | INFO | src.logging_config | Starting Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
      "2025-12-17 15:27:11,930 | INFO | src.logging_config | Database environment variables loaded successfully\n",
      "2025-12-17 15:27:12,089 | INFO | src.logging_config | PostgreSQL engine constructed\n",
      "2025-12-17 15:27:12,132 | INFO | src.logging_config | Discovered 204 standardized Parquet files\n",
      "2025-12-17 15:27:12,293 | INFO | src.logging_config | Existing tables in database: ['track_status', 'laps', 'drivers', 'races']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
      "------------------------------------------------------------\n",
      "This notebook will:\n",
      "  - Load standardized Parquet into PostgreSQL\n",
      "  - Define relational schemas and constraints\n",
      "  - Create derived invariants (cumulative time, pit markers)\n",
      "  - Prepare data for deterministic undercut detection\n",
      "\n",
      "This notebook will not:\n",
      "  - Call external APIs\n",
      "  - Perform semantic standardization\n",
      "  - Detect undercut events\n",
      "  - Produce analytical conclusions\n",
      "\n",
      "Notebook 02 outputs are treated as authoritative.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 1: Environment setup and execution preconditions\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Resolve project root dynamically (Jupyter-safe)\n",
    "# ------------------------------------------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(start_path: Path) -> Path:\n",
    "    current = start_path.resolve()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"src\").exists():\n",
    "            return parent\n",
    "    raise RuntimeError(\n",
    "        \"Could not locate project root. Ensure notebook is inside the project.\"\n",
    "    )\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Standard library and third-party imports\n",
    "# ------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Project imports\n",
    "# ------------------------------------------------------------\n",
    "from src.config import Config\n",
    "from src.logging_config import setup_logging\n",
    "from src.utils import ensure_dir\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Initialize logging\n",
    "# ------------------------------------------------------------\n",
    "logger, error_logger = setup_logging()\n",
    "logger.info(\"Starting Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load environment variables (.env)\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "missing_env = [\n",
    "    var for var in [\n",
    "        \"DB_USER\", \"DB_PASSWORD\", \"DB_HOST\", \"DB_PORT\", \"DB_NAME\"\n",
    "    ] if os.getenv(var) is None\n",
    "]\n",
    "\n",
    "if missing_env:\n",
    "    raise EnvironmentError(\n",
    "        f\"Missing required database environment variables: {missing_env}\"\n",
    "    )\n",
    "\n",
    "logger.info(\"Database environment variables loaded successfully\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create PostgreSQL engine (no connection yet)\n",
    "# ------------------------------------------------------------\n",
    "DATABASE_URL = (\n",
    "    f\"postgresql://{DB_USER}:{DB_PASSWORD}\"\n",
    "    f\"@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "logger.info(\"PostgreSQL engine constructed\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Resolve standardized Parquet input directory\n",
    "# ------------------------------------------------------------\n",
    "STANDARDIZED_DATA_DIR = Config.DATA_DIR / \"interim\" / \"standardized\"\n",
    "\n",
    "if not STANDARDIZED_DATA_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Standardized Parquet directory does not exist. \"\n",
    "        \"Notebook 02 must be executed successfully before Notebook 03.\"\n",
    "    )\n",
    "\n",
    "standardized_files = list(STANDARDIZED_DATA_DIR.rglob(\"*.parquet\"))\n",
    "\n",
    "if not standardized_files:\n",
    "    raise FileNotFoundError(\n",
    "        \"No standardized Parquet files found. \"\n",
    "        \"Notebook 02 did not produce usable outputs.\"\n",
    "    )\n",
    "\n",
    "logger.info(\n",
    "    f\"Discovered {len(standardized_files)} standardized Parquet files\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Inspect database state (read-only)\n",
    "# ------------------------------------------------------------\n",
    "inspector = inspect(engine)\n",
    "existing_tables = inspector.get_table_names()\n",
    "\n",
    "logger.info(f\"Existing tables in database: {existing_tables}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Declare execution scope\n",
    "# ------------------------------------------------------------\n",
    "print(\"\"\"\n",
    "============================================================\n",
    "Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "------------------------------------------------------------\n",
    "This notebook will:\n",
    "  - Load standardized Parquet into PostgreSQL\n",
    "  - Define relational schemas and constraints\n",
    "  - Create derived invariants (cumulative time, pit markers)\n",
    "  - Prepare data for deterministic undercut detection\n",
    "\n",
    "This notebook will not:\n",
    "  - Call external APIs\n",
    "  - Perform semantic standardization\n",
    "  - Detect undercut events\n",
    "  - Produce analytical conclusions\n",
    "\n",
    "Notebook 02 outputs are treated as authoritative.\n",
    "============================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9d892e9-9273-4730-a793-4e7bd9c49380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:27:12,331 | INFO | src.logging_config | Defining relational schema with explicit identity and grain\n",
      "2025-12-17 15:27:12,360 | INFO | src.logging_config | Relational schema defined with explicit keys and constraints\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 2: Relational schema definition (corrected)\n",
    "# ============================================================\n",
    "\n",
    "from sqlalchemy import (\n",
    "    Table, Column, Integer, BigInteger, String,\n",
    "    Boolean, MetaData, ForeignKey, ForeignKeyConstraint,\n",
    "    Index, PrimaryKeyConstraint, CheckConstraint\n",
    ")\n",
    "\n",
    "logger.info(\"Defining relational schema with explicit identity and grain\")\n",
    "\n",
    "metadata = MetaData()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RACES\n",
    "# One row per race\n",
    "# race_id = \"{season}_{round}\"\n",
    "# ------------------------------------------------------------\n",
    "races = Table(\n",
    "    \"races\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, primary_key=True),\n",
    "    Column(\"season\", Integer, nullable=False),\n",
    "    Column(\"round\", Integer, nullable=False),\n",
    "\n",
    "    CheckConstraint(\"season >= 1950\", name=\"ck_races_valid_season\"),\n",
    "    CheckConstraint(\"round > 0\", name=\"ck_races_valid_round\"),\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DRIVERS\n",
    "# One row per driver per race (race-scoped identity)\n",
    "# driver_id = \"{race_id}_{driver_code}\"\n",
    "# ------------------------------------------------------------\n",
    "drivers = Table(\n",
    "    \"drivers\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, ForeignKey(\"races.race_id\"), nullable=False),\n",
    "    Column(\"driver_code\", String, nullable=False),\n",
    "    Column(\"driver_number\", Integer, nullable=False),\n",
    "\n",
    "    PrimaryKeyConstraint(\n",
    "        \"race_id\",\n",
    "        \"driver_code\",\n",
    "        name=\"pk_drivers\"\n",
    "    ),\n",
    "\n",
    "    CheckConstraint(\n",
    "        \"driver_number > 0\",\n",
    "        name=\"ck_drivers_valid_number\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LAPS\n",
    "# Core fact table\n",
    "# One row per (race, driver, lap)\n",
    "# ------------------------------------------------------------\n",
    "laps = Table(\n",
    "    \"laps\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, ForeignKey(\"races.race_id\"), nullable=False),\n",
    "    Column(\"driver_code\", String, nullable=False),\n",
    "    Column(\"lap_number\", Integer, nullable=False),\n",
    "\n",
    "    # Standardized fields\n",
    "    Column(\"lap_time_ms\", BigInteger, nullable=True),\n",
    "    Column(\"tyre_compound\", String, nullable=True),\n",
    "\n",
    "    # Relational invariants (computed later)\n",
    "    Column(\"cumulative_time_ms\", BigInteger, nullable=True),\n",
    "    Column(\"is_pit_lap\", Boolean, nullable=True),\n",
    "    Column(\"is_out_lap\", Boolean, nullable=True),\n",
    "\n",
    "    PrimaryKeyConstraint(\n",
    "        \"race_id\",\n",
    "        \"driver_code\",\n",
    "        \"lap_number\",\n",
    "        name=\"pk_laps\"\n",
    "    ),\n",
    "\n",
    "    ForeignKeyConstraint(\n",
    "        [\"race_id\", \"driver_code\"],\n",
    "        [\"drivers.race_id\", \"drivers.driver_code\"],\n",
    "        name=\"fk_laps_drivers\"\n",
    "    ),\n",
    "\n",
    "    CheckConstraint(\"lap_number > 0\", name=\"ck_laps_valid_lap\"),\n",
    ")\n",
    "\n",
    "Index(\"idx_laps_race_driver\", laps.c.race_id, laps.c.driver_code)\n",
    "Index(\"idx_laps_race_lap\", laps.c.race_id, laps.c.lap_number)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TRACK STATUS\n",
    "# One row per lap per race\n",
    "# ------------------------------------------------------------\n",
    "track_status = Table(\n",
    "    \"track_status\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, ForeignKey(\"races.race_id\"), nullable=False),\n",
    "    Column(\"lap_number\", Integer, nullable=False),\n",
    "    Column(\"track_status\", String, nullable=False),\n",
    "\n",
    "    PrimaryKeyConstraint(\n",
    "        \"race_id\",\n",
    "        \"lap_number\",\n",
    "        name=\"pk_track_status\"\n",
    "    ),\n",
    "\n",
    "    CheckConstraint(\"lap_number > 0\", name=\"ck_track_status_valid_lap\"),\n",
    ")\n",
    "\n",
    "Index(\"idx_track_status_race_lap\", track_status.c.race_id, track_status.c.lap_number)\n",
    "\n",
    "logger.info(\"Relational schema defined with explicit keys and constraints\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852792d8-2817-4941-a444-7488ffc5e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:27:12,384 | INFO | src.logging_config | Executing DDL to ensure PostgreSQL schema exists\n",
      "2025-12-17 15:27:12,390 | INFO | src.logging_config | Schema already exists with expected tables ‚Äî skipping DDL execution\n",
      "2025-12-17 15:27:12,393 | INFO | src.logging_config | Verified schema tables: ['drivers', 'laps', 'races', 'track_status']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Notebook 03 ‚Äî Schema Ready\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ Required tables exist\n",
      "‚Ä¢ Primary and foreign keys enforced\n",
      "‚Ä¢ Safe to proceed to data loading\n",
      "\n",
      "This cell is idempotent and restart-safe.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 3: Execute DDL (idempotent & safe)\n",
    "# ============================================================\n",
    "\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "logger.info(\"Executing DDL to ensure PostgreSQL schema exists\")\n",
    "\n",
    "inspector = inspect(engine)\n",
    "existing_tables = set(inspector.get_table_names())\n",
    "\n",
    "expected_tables = {\"races\", \"drivers\", \"laps\", \"track_status\"}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Case 1: Database is empty ‚Üí create schema\n",
    "# ------------------------------------------------------------\n",
    "if not existing_tables:\n",
    "    logger.info(\"Database is empty ‚Äî creating schema\")\n",
    "    metadata.create_all(engine)\n",
    "    logger.info(\"Schema created successfully\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Case 2: Expected tables already exist ‚Üí skip creation\n",
    "# ------------------------------------------------------------\n",
    "elif existing_tables == expected_tables:\n",
    "    logger.info(\n",
    "        \"Schema already exists with expected tables ‚Äî skipping DDL execution\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Case 3: Unexpected tables present ‚Üí fail loudly\n",
    "# ------------------------------------------------------------\n",
    "else:\n",
    "    raise RuntimeError(\n",
    "        f\"Database contains unexpected tables: {sorted(existing_tables)}. \"\n",
    "        f\"Expected exactly: {sorted(expected_tables)}. \"\n",
    "        \"Manual intervention required.\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Final verification\n",
    "# ------------------------------------------------------------\n",
    "final_tables = set(inspector.get_table_names())\n",
    "missing = expected_tables - final_tables\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        f\"Schema verification failed. Missing tables: {missing}\"\n",
    "    )\n",
    "\n",
    "logger.info(f\"Verified schema tables: {sorted(final_tables)}\")\n",
    "\n",
    "print(\"\"\"\n",
    "============================================================\n",
    "Notebook 03 ‚Äî Schema Ready\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ Required tables exist\n",
    "‚Ä¢ Primary and foreign keys enforced\n",
    "‚Ä¢ Safe to proceed to data loading\n",
    "\n",
    "This cell is idempotent and restart-safe.\n",
    "============================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce0d2c83-001a-44f6-a263-3bb9aa88e56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:27:12,420 | INFO | src.logging_config | Starting standardized Parquet schema diagnostics\n",
      "2025-12-17 15:27:14,979 | INFO | src.logging_config | Inspected 204 standardized Parquet files\n",
      "2025-12-17 15:27:15,029 | INFO | src.logging_config | Schema snapshot written to C:\\Users\\hersh\\Desktop\\f1_analysis_project\\data\\interim\\standardized_schema_snapshot.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Notebook 03 ‚Äî Standardized Parquet Diagnostics Complete\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ All standardized Parquet files inspected\n",
      "‚Ä¢ Exact schemas captured per table per race\n",
      "‚Ä¢ No assumptions made about race identity\n",
      "‚Ä¢ Schema snapshot persisted for loader design\n",
      "\n",
      "Next step:\n",
      "  - Inspect standardized_schema_snapshot.json\n",
      "  - Decide authoritative race identity source\n",
      "  - Design robust loading logic (Cell 5)\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 4: Standardized Parquet Schema Diagnostics\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "from src.config import BASE_DIR, Config\n",
    "from src.logging_config import setup_logging\n",
    "\n",
    "logger, _ = setup_logging()\n",
    "logger.info(\"Starting standardized Parquet schema diagnostics\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Resolve paths using actual config contract\n",
    "# ------------------------------------------------------------\n",
    "PROJECT_ROOT = BASE_DIR\n",
    "INTERIM_DATA_DIR = Config.DATA_DIR / \"interim\"\n",
    "STANDARDIZED_SCHEMA_DIR = INTERIM_DATA_DIR / \"standardized\"\n",
    "SCHEMA_OUTPUT_PATH = INTERIM_DATA_DIR / \"standardized_schema_snapshot.json\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Validate directory exists\n",
    "# ------------------------------------------------------------\n",
    "if not STANDARDIZED_SCHEMA_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Standardized data directory not found: {STANDARDIZED_SCHEMA_DIR}\"\n",
    "    )\n",
    "\n",
    "schema_snapshot = defaultdict(dict)\n",
    "file_count = 0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Inspect all standardized parquet files\n",
    "# ------------------------------------------------------------\n",
    "for parquet_path in STANDARDIZED_SCHEMA_DIR.rglob(\"*.parquet\"):\n",
    "    race_dir = parquet_path.parent\n",
    "    table_name = parquet_path.stem\n",
    "\n",
    "    season_folder = race_dir.parts[-2]   # e.g. year=2024\n",
    "    race_folder = race_dir.name           # e.g. round=10_Spanish_Grand_Prix\n",
    "    race_key = f\"{season_folder}/{race_folder}\"\n",
    "\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    schema_snapshot.setdefault(table_name, {})\n",
    "    schema_snapshot[table_name][race_key] = {\n",
    "        \"columns\": list(df.columns),\n",
    "        \"dtypes\": {col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "        \"row_count\": int(len(df)),\n",
    "    }\n",
    "\n",
    "    file_count += 1\n",
    "\n",
    "logger.info(f\"Inspected {file_count} standardized Parquet files\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Persist schema snapshot\n",
    "# ------------------------------------------------------------\n",
    "with open(SCHEMA_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema_snapshot, f, indent=2)\n",
    "\n",
    "logger.info(f\"Schema snapshot written to {SCHEMA_OUTPUT_PATH}\")\n",
    "\n",
    "print(\"\"\"\n",
    "============================================================\n",
    "Notebook 03 ‚Äî Standardized Parquet Diagnostics Complete\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ All standardized Parquet files inspected\n",
    "‚Ä¢ Exact schemas captured per table per race\n",
    "‚Ä¢ No assumptions made about race identity\n",
    "‚Ä¢ Schema snapshot persisted for loader design\n",
    "\n",
    "Next step:\n",
    "  - Inspect standardized_schema_snapshot.json\n",
    "  - Decide authoritative race identity source\n",
    "  - Design robust loading logic (Cell 5)\n",
    "============================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f85dd27-e9a9-4559-b2ac-52644c00ba21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:27:15,063 | INFO | src.logging_config | Starting standardized Parquet -> PostgreSQL load (idempotent)\n",
      "2025-12-17 15:27:15,070 | INFO | src.logging_config | Processing race 2022_10\n",
      "2025-12-17 15:27:15,077 | INFO | src.logging_config | Race 2022_10 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,080 | INFO | src.logging_config | Processing race 2022_11\n",
      "2025-12-17 15:27:15,084 | INFO | src.logging_config | Race 2022_11 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,085 | INFO | src.logging_config | Processing race 2022_12\n",
      "2025-12-17 15:27:15,089 | INFO | src.logging_config | Race 2022_12 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,090 | INFO | src.logging_config | Processing race 2022_13\n",
      "2025-12-17 15:27:15,094 | INFO | src.logging_config | Race 2022_13 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,097 | INFO | src.logging_config | Processing race 2022_14\n",
      "2025-12-17 15:27:15,100 | INFO | src.logging_config | Race 2022_14 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,102 | INFO | src.logging_config | Processing race 2022_15\n",
      "2025-12-17 15:27:15,105 | INFO | src.logging_config | Race 2022_15 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,108 | INFO | src.logging_config | Processing race 2022_16\n",
      "2025-12-17 15:27:15,110 | INFO | src.logging_config | Race 2022_16 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,112 | INFO | src.logging_config | Processing race 2022_17\n",
      "2025-12-17 15:27:15,116 | INFO | src.logging_config | Race 2022_17 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,118 | INFO | src.logging_config | Processing race 2022_18\n",
      "2025-12-17 15:27:15,120 | INFO | src.logging_config | Race 2022_18 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,122 | INFO | src.logging_config | Processing race 2022_19\n",
      "2025-12-17 15:27:15,125 | INFO | src.logging_config | Race 2022_19 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,127 | INFO | src.logging_config | Processing race 2022_1\n",
      "2025-12-17 15:27:15,131 | INFO | src.logging_config | Race 2022_1 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,132 | INFO | src.logging_config | Processing race 2022_20\n",
      "2025-12-17 15:27:15,136 | INFO | src.logging_config | Race 2022_20 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,138 | INFO | src.logging_config | Processing race 2022_21\n",
      "2025-12-17 15:27:15,141 | INFO | src.logging_config | Race 2022_21 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,143 | INFO | src.logging_config | Processing race 2022_22\n",
      "2025-12-17 15:27:15,146 | INFO | src.logging_config | Race 2022_22 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,149 | INFO | src.logging_config | Processing race 2022_2\n",
      "2025-12-17 15:27:15,151 | INFO | src.logging_config | Race 2022_2 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,154 | INFO | src.logging_config | Processing race 2022_3\n",
      "2025-12-17 15:27:15,157 | INFO | src.logging_config | Race 2022_3 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,159 | INFO | src.logging_config | Processing race 2022_4\n",
      "2025-12-17 15:27:15,162 | INFO | src.logging_config | Race 2022_4 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,164 | INFO | src.logging_config | Processing race 2022_5\n",
      "2025-12-17 15:27:15,166 | INFO | src.logging_config | Race 2022_5 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,168 | INFO | src.logging_config | Processing race 2022_6\n",
      "2025-12-17 15:27:15,172 | INFO | src.logging_config | Race 2022_6 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,174 | INFO | src.logging_config | Processing race 2022_7\n",
      "2025-12-17 15:27:15,177 | INFO | src.logging_config | Race 2022_7 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,180 | INFO | src.logging_config | Processing race 2022_8\n",
      "2025-12-17 15:27:15,182 | INFO | src.logging_config | Race 2022_8 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,183 | INFO | src.logging_config | Processing race 2022_9\n",
      "2025-12-17 15:27:15,186 | INFO | src.logging_config | Race 2022_9 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,190 | INFO | src.logging_config | Processing race 2023_10\n",
      "2025-12-17 15:27:15,193 | INFO | src.logging_config | Race 2023_10 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,195 | INFO | src.logging_config | Processing race 2023_11\n",
      "2025-12-17 15:27:15,198 | INFO | src.logging_config | Race 2023_11 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,199 | INFO | src.logging_config | Processing race 2023_12\n",
      "2025-12-17 15:27:15,203 | INFO | src.logging_config | Race 2023_12 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,205 | INFO | src.logging_config | Processing race 2023_13\n",
      "2025-12-17 15:27:15,206 | INFO | src.logging_config | Race 2023_13 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,208 | INFO | src.logging_config | Processing race 2023_14\n",
      "2025-12-17 15:27:15,210 | INFO | src.logging_config | Race 2023_14 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,213 | INFO | src.logging_config | Processing race 2023_15\n",
      "2025-12-17 15:27:15,215 | INFO | src.logging_config | Race 2023_15 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,216 | INFO | src.logging_config | Processing race 2023_16\n",
      "2025-12-17 15:27:15,218 | INFO | src.logging_config | Race 2023_16 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,219 | INFO | src.logging_config | Processing race 2023_17\n",
      "2025-12-17 15:27:15,221 | INFO | src.logging_config | Race 2023_17 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,223 | INFO | src.logging_config | Processing race 2023_18\n",
      "2025-12-17 15:27:15,225 | INFO | src.logging_config | Race 2023_18 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,227 | INFO | src.logging_config | Processing race 2023_19\n",
      "2025-12-17 15:27:15,231 | INFO | src.logging_config | Race 2023_19 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,232 | INFO | src.logging_config | Processing race 2023_1\n",
      "2025-12-17 15:27:15,235 | INFO | src.logging_config | Race 2023_1 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,238 | INFO | src.logging_config | Processing race 2023_20\n",
      "2025-12-17 15:27:15,241 | INFO | src.logging_config | Race 2023_20 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,243 | INFO | src.logging_config | Processing race 2023_21\n",
      "2025-12-17 15:27:15,246 | INFO | src.logging_config | Race 2023_21 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,247 | INFO | src.logging_config | Processing race 2023_22\n",
      "2025-12-17 15:27:15,250 | INFO | src.logging_config | Race 2023_22 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,252 | INFO | src.logging_config | Processing race 2023_2\n",
      "2025-12-17 15:27:15,255 | INFO | src.logging_config | Race 2023_2 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,258 | INFO | src.logging_config | Processing race 2023_3\n",
      "2025-12-17 15:27:15,260 | INFO | src.logging_config | Race 2023_3 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,261 | INFO | src.logging_config | Processing race 2023_4\n",
      "2025-12-17 15:27:15,264 | INFO | src.logging_config | Race 2023_4 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,266 | INFO | src.logging_config | Processing race 2023_5\n",
      "2025-12-17 15:27:15,267 | INFO | src.logging_config | Race 2023_5 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,269 | INFO | src.logging_config | Processing race 2023_6\n",
      "2025-12-17 15:27:15,273 | INFO | src.logging_config | Race 2023_6 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,274 | INFO | src.logging_config | Processing race 2023_7\n",
      "2025-12-17 15:27:15,276 | INFO | src.logging_config | Race 2023_7 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,278 | INFO | src.logging_config | Processing race 2023_8\n",
      "2025-12-17 15:27:15,281 | INFO | src.logging_config | Race 2023_8 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,283 | INFO | src.logging_config | Processing race 2023_9\n",
      "2025-12-17 15:27:15,284 | INFO | src.logging_config | Race 2023_9 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,288 | INFO | src.logging_config | Processing race 2024_10\n",
      "2025-12-17 15:27:15,291 | INFO | src.logging_config | Race 2024_10 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,293 | INFO | src.logging_config | Processing race 2024_11\n",
      "2025-12-17 15:27:15,295 | INFO | src.logging_config | Race 2024_11 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,297 | INFO | src.logging_config | Processing race 2024_12\n",
      "2025-12-17 15:27:15,299 | INFO | src.logging_config | Race 2024_12 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,301 | INFO | src.logging_config | Processing race 2024_13\n",
      "2025-12-17 15:27:15,305 | INFO | src.logging_config | Race 2024_13 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,306 | INFO | src.logging_config | Processing race 2024_14\n",
      "2025-12-17 15:27:15,310 | INFO | src.logging_config | Race 2024_14 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,311 | INFO | src.logging_config | Processing race 2024_15\n",
      "2025-12-17 15:27:15,313 | INFO | src.logging_config | Race 2024_15 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,315 | INFO | src.logging_config | Processing race 2024_16\n",
      "2025-12-17 15:27:15,318 | INFO | src.logging_config | Race 2024_16 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,320 | INFO | src.logging_config | Processing race 2024_17\n",
      "2025-12-17 15:27:15,323 | INFO | src.logging_config | Race 2024_17 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,325 | INFO | src.logging_config | Processing race 2024_18\n",
      "2025-12-17 15:27:15,328 | INFO | src.logging_config | Race 2024_18 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,331 | INFO | src.logging_config | Processing race 2024_19\n",
      "2025-12-17 15:27:15,333 | INFO | src.logging_config | Race 2024_19 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,335 | INFO | src.logging_config | Processing race 2024_1\n",
      "2025-12-17 15:27:15,339 | INFO | src.logging_config | Race 2024_1 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,341 | INFO | src.logging_config | Processing race 2024_20\n",
      "2025-12-17 15:27:15,344 | INFO | src.logging_config | Race 2024_20 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,345 | INFO | src.logging_config | Processing race 2024_21\n",
      "2025-12-17 15:27:15,348 | INFO | src.logging_config | Race 2024_21 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,349 | INFO | src.logging_config | Processing race 2024_22\n",
      "2025-12-17 15:27:15,353 | INFO | src.logging_config | Race 2024_22 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,355 | INFO | src.logging_config | Processing race 2024_23\n",
      "2025-12-17 15:27:15,358 | INFO | src.logging_config | Race 2024_23 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,360 | INFO | src.logging_config | Processing race 2024_24\n",
      "2025-12-17 15:27:15,363 | INFO | src.logging_config | Race 2024_24 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,365 | INFO | src.logging_config | Processing race 2024_2\n",
      "2025-12-17 15:27:15,368 | INFO | src.logging_config | Race 2024_2 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,370 | INFO | src.logging_config | Processing race 2024_3\n",
      "2025-12-17 15:27:15,373 | INFO | src.logging_config | Race 2024_3 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,374 | INFO | src.logging_config | Processing race 2024_4\n",
      "2025-12-17 15:27:15,378 | INFO | src.logging_config | Race 2024_4 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,380 | INFO | src.logging_config | Processing race 2024_5\n",
      "2025-12-17 15:27:15,383 | INFO | src.logging_config | Race 2024_5 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,384 | INFO | src.logging_config | Processing race 2024_6\n",
      "2025-12-17 15:27:15,387 | INFO | src.logging_config | Race 2024_6 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,389 | INFO | src.logging_config | Processing race 2024_7\n",
      "2025-12-17 15:27:15,393 | INFO | src.logging_config | Race 2024_7 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,394 | INFO | src.logging_config | Processing race 2024_8\n",
      "2025-12-17 15:27:15,397 | INFO | src.logging_config | Race 2024_8 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,399 | INFO | src.logging_config | Processing race 2024_9\n",
      "2025-12-17 15:27:15,400 | INFO | src.logging_config | Race 2024_9 already loaded ‚Äî skipping\n",
      "2025-12-17 15:27:15,402 | INFO | src.logging_config | All eligible standardized Parquet files loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Notebook 03 ‚Äî Data Load Complete (Idempotent)\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ Existing races safely skipped\n",
      "‚Ä¢ No duplicate inserts possible\n",
      "‚Ä¢ Relational invariants preserved\n",
      "‚Ä¢ Track status intentionally deferred\n",
      "\n",
      "Notebook 03 is now environment-agnostic.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 5: Idempotent Standardized Parquet ‚Üí PostgreSQL Load\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "from src.logging_config import setup_logging\n",
    "from src.config import Config\n",
    "\n",
    "logger, _ = setup_logging()\n",
    "logger.info(\"Starting standardized Parquet -> PostgreSQL load (idempotent)\")\n",
    "\n",
    "STANDARDIZED_DIR = Config.DATA_DIR / \"interim\" / \"standardized\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "\n",
    "    for year_dir in sorted(STANDARDIZED_DIR.glob(\"year=*\")):\n",
    "        season = int(year_dir.name.split(\"=\")[1])\n",
    "\n",
    "        for race_dir in sorted(year_dir.iterdir()):\n",
    "            race_id = race_dir.name.split(\"_\")[0].replace(\"round=\", \"\")\n",
    "            race_id = f\"{season}_{race_id}\"\n",
    "\n",
    "            logger.info(f\"Processing race {race_id}\")\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # Skip race if already loaded\n",
    "            # ------------------------------------------------\n",
    "            race_exists = conn.execute(\n",
    "                text(\"SELECT 1 FROM races WHERE race_id = :race_id\"),\n",
    "                {\"race_id\": race_id}\n",
    "            ).first()\n",
    "\n",
    "            if race_exists:\n",
    "                logger.info(f\"Race {race_id} already loaded ‚Äî skipping\")\n",
    "                continue\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # Load race metadata (authoritative from directory)\n",
    "            # ------------------------------------------------\n",
    "            round_number = int(race_dir.name.split(\"_\")[0].split(\"=\")[1])\n",
    "\n",
    "            race_df = pd.DataFrame([{\n",
    "                \"race_id\": race_id,\n",
    "                \"season\": season,\n",
    "                \"round\": round_number,\n",
    "            }])\n",
    "\n",
    "            race_df.to_sql(\n",
    "                \"races\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # Drivers (from results.parquet)\n",
    "            # ------------------------------------------------\n",
    "            results_df = pd.read_parquet(race_dir / \"results.parquet\")\n",
    "\n",
    "            drivers_df = (\n",
    "                results_df[[\"Abbreviation\", \"DriverNumber\"]]\n",
    "                .drop_duplicates()\n",
    "                .rename(columns={\n",
    "                    \"Abbreviation\": \"driver_code\",\n",
    "                    \"DriverNumber\": \"driver_number\",\n",
    "                })\n",
    "            )\n",
    "\n",
    "            drivers_df[\"race_id\"] = race_id\n",
    "\n",
    "            drivers_df.to_sql(\n",
    "                \"drivers\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # Laps\n",
    "            # ------------------------------------------------\n",
    "            laps_df = pd.read_parquet(race_dir / \"laps.parquet\")\n",
    "\n",
    "            laps_df = laps_df.rename(columns={\n",
    "                \"Driver\": \"driver_code\",\n",
    "                \"LapNumber\": \"lap_number\",\n",
    "            })\n",
    "\n",
    "            laps_df[\"race_id\"] = race_id\n",
    "\n",
    "            laps_df = laps_df[\n",
    "                [\n",
    "                    \"race_id\",\n",
    "                    \"driver_code\",\n",
    "                    \"lap_number\",\n",
    "                    \"lap_time_ms\",\n",
    "                    \"tyre_compound\",\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "            logger.info(f\"Laps columns: {list(laps_df.columns)}\")\n",
    "\n",
    "            laps_df.to_sql(\n",
    "                \"laps\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "logger.info(\"All eligible standardized Parquet files loaded successfully\")\n",
    "\n",
    "print(\"\"\"\n",
    "============================================================\n",
    "Notebook 03 ‚Äî Data Load Complete (Idempotent)\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ Existing races safely skipped\n",
    "‚Ä¢ No duplicate inserts possible\n",
    "‚Ä¢ Relational invariants preserved\n",
    "‚Ä¢ Track status intentionally deferred\n",
    "\n",
    "Notebook 03 is now environment-agnostic.\n",
    "============================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7919023-e411-4e5f-bcea-730c46a5a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-17 15:27:15,434 | INFO | src.logging_config | Ellipsis\n",
      "2025-12-17 15:27:15,453 | INFO | src.logging_config | Row counts ‚Äî races=68, drivers=1359, laps=74605\n",
      "2025-12-17 15:27:15,484 | INFO | src.logging_config | No orphan laps detected\n",
      "2025-12-17 15:27:15,607 | INFO | src.logging_config | Lap grain verified (race_id, driver_code, lap_number)\n",
      "2025-12-17 15:27:15,622 | INFO | src.logging_config | Standardized track_status.parquet present for all races\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Notebook 03 ‚Äî Relational Validation Complete\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ Core relational tables populated\n",
      "‚Ä¢ Referential integrity enforced\n",
      "‚Ä¢ Lap grain is correct and unique\n",
      "‚Ä¢ Track status events available on disk\n",
      "‚Ä¢ Database ready for derived feature computation\n",
      "\n",
      "Next Notebook:\n",
      "  - Compute cumulative lap times\n",
      "  - Derive pit laps and out laps\n",
      "  - Map track status events to lap-level flags\n",
      "  - Enable deterministic undercut detection\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 6: Load Validation & Pipeline Readiness\n",
    "# ============================================================\n",
    "\n",
    "from sqlalchemy import text\n",
    "from src.logging_config import setup_logging\n",
    "from src.config import Config\n",
    "\n",
    "logger, _ = setup_logging()\n",
    "logger.info(...)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Database-level validation\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1. Basic row counts (sanity, not analytics)\n",
    "    # --------------------------------------------------------\n",
    "    counts = conn.execute(text(\"\"\"\n",
    "        SELECT\n",
    "            (SELECT COUNT(*) FROM races)   AS races,\n",
    "            (SELECT COUNT(*) FROM drivers) AS drivers,\n",
    "            (SELECT COUNT(*) FROM laps)    AS laps\n",
    "    \"\"\")).mappings().one()\n",
    "\n",
    "    logger.info(\n",
    "        f\"Row counts ‚Äî races={counts['races']}, \"\n",
    "        f\"drivers={counts['drivers']}, \"\n",
    "        f\"laps={counts['laps']}\"\n",
    "    )\n",
    "\n",
    "    if counts[\"races\"] == 0:\n",
    "        raise RuntimeError(\"No races loaded ‚Äî pipeline is broken\")\n",
    "\n",
    "    if counts[\"drivers\"] == 0:\n",
    "        raise RuntimeError(\"No drivers loaded ‚Äî identity mapping failed\")\n",
    "\n",
    "    if counts[\"laps\"] == 0:\n",
    "        raise RuntimeError(\"No laps loaded ‚Äî core fact table empty\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2. Referential integrity: laps ‚Üí drivers\n",
    "    # --------------------------------------------------------\n",
    "    orphan_laps = conn.execute(text(\"\"\"\n",
    "        SELECT COUNT(*) AS orphan_count\n",
    "        FROM laps l\n",
    "        LEFT JOIN drivers d\n",
    "          ON l.race_id = d.race_id\n",
    "         AND l.driver_code = d.driver_code\n",
    "        WHERE d.driver_code IS NULL\n",
    "    \"\"\")).scalar()\n",
    "\n",
    "    if orphan_laps > 0:\n",
    "        raise RuntimeError(\n",
    "            f\"Found {orphan_laps} laps without matching drivers\"\n",
    "        )\n",
    "\n",
    "    logger.info(\"No orphan laps detected\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3. Lap grain validation\n",
    "    # --------------------------------------------------------\n",
    "    duplicate_laps = conn.execute(text(\"\"\"\n",
    "        SELECT COUNT(*) FROM (\n",
    "            SELECT race_id, driver_code, lap_number, COUNT(*) c\n",
    "            FROM laps\n",
    "            GROUP BY race_id, driver_code, lap_number\n",
    "            HAVING COUNT(*) > 1\n",
    "        ) t\n",
    "    \"\"\")).scalar()\n",
    "\n",
    "    if duplicate_laps > 0:\n",
    "        raise RuntimeError(\n",
    "            f\"Duplicate lap rows detected: {duplicate_laps}\"\n",
    "        )\n",
    "\n",
    "    logger.info(\"Lap grain verified (race_id, driver_code, lap_number)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Filesystem-level validation for deferred datasets\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "STANDARDIZED_DIR = Config.DATA_DIR / \"interim\" / \"standardized\"\n",
    "\n",
    "missing_track_status = []\n",
    "\n",
    "for year_dir in STANDARDIZED_DIR.glob(\"year=*\"):\n",
    "    for race_dir in year_dir.iterdir():\n",
    "        if not (race_dir / \"track_status.parquet\").exists():\n",
    "            missing_track_status.append(str(race_dir))\n",
    "\n",
    "if missing_track_status:\n",
    "    raise RuntimeError(\n",
    "        \"Standardized track_status.parquet missing for races:\\n\"\n",
    "        + \"\\n\".join(missing_track_status[:5])\n",
    "        + (\"\\n...\" if len(missing_track_status) > 5 else \"\")\n",
    "    )\n",
    "\n",
    "logger.info(\"Standardized track_status.parquet present for all races\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Final readiness confirmation\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\"\"\n",
    "============================================================\n",
    "Notebook 03 ‚Äî Relational Validation Complete\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ Core relational tables populated\n",
    "‚Ä¢ Referential integrity enforced\n",
    "‚Ä¢ Lap grain is correct and unique\n",
    "‚Ä¢ Track status events available on disk\n",
    "‚Ä¢ Database ready for derived feature computation\n",
    "\n",
    "Next Notebook:\n",
    "  - Compute cumulative lap times\n",
    "  - Derive pit laps and out laps\n",
    "  - Map track status events to lap-level flags\n",
    "  - Enable deterministic undercut detection\n",
    "============================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ac8dac-b36c-4733-b4eb-ac67f644d2ec",
   "metadata": {},
   "source": [
    "# üß† Notebook 03 ‚Äî Conclusion, Error Log & Forward Plan\n",
    "\n",
    "## üßæ High-Level Summary\n",
    "\n",
    "Notebook 03 was **not a smooth execution notebook** ‚Äî and that is precisely why it is one of the most important ones.\n",
    "\n",
    "What appeared initially as a simple ‚ÄúParquet ‚Üí PostgreSQL load‚Äù revealed:\n",
    "\n",
    "- Hidden schema assumptions\n",
    "- Grain mismatches\n",
    "- Logging misconfigurations\n",
    "- Idempotency pitfalls\n",
    "- Incorrect mental models about track status data\n",
    "\n",
    "Each of these was encountered, diagnosed, and resolved deliberately.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç What We Built (Final State)\n",
    "\n",
    "At completion, we achieved:\n",
    "\n",
    "üìä **Relational Tables**\n",
    "- `races`\n",
    "- `drivers`\n",
    "- `laps`\n",
    "\n",
    "üîí **Guaranteed Invariants**\n",
    "- No duplicate races\n",
    "- No duplicate drivers per race\n",
    "- No duplicate laps\n",
    "- No orphan records\n",
    "\n",
    "üîÅ **Idempotent Execution**\n",
    "- Kernel restarts are safe\n",
    "- Full notebook re-runs are safe\n",
    "- Existing data is detected and skipped\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Errors & Problems Encountered (Chronological)\n",
    "\n",
    "### ‚ùå Column Name Assumptions\n",
    "- Expected `driver_code`, found `Driver` / `Abbreviation`\n",
    "- Resolved by inspecting actual Parquet schemas\n",
    "\n",
    "### ‚ùå NOT NULL Violations\n",
    "- Missing `round` field in `races`\n",
    "- Fixed by parsing race identity correctly\n",
    "\n",
    "### ‚ùå Duplicate Primary Keys\n",
    "- Occurred when rerunning inserts\n",
    "- Fixed via `ON CONFLICT DO NOTHING`\n",
    "\n",
    "### ‚ùå Track Status Grain Errors\n",
    "- Attempted early loading caused null lap numbers\n",
    "- Realized track status is **not lap-level**\n",
    "\n",
    "### ‚ùå Logger Misuse\n",
    "- `setup_logging()` returned `(logger, handler)`\n",
    "- Incorrect unpacking caused runtime failures\n",
    "- Fixed by explicit tuple handling\n",
    "\n",
    "### ‚ùå Scope Errors in Logging\n",
    "- Logging variables after conditional skips\n",
    "- Fixed by removing unsafe debug lines\n",
    "\n",
    "Each error refined the pipeline ‚Äî none were ignored.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Validation Results (Cell 6)\n",
    "\n",
    "Final checks confirmed:\n",
    "\n",
    "üìà Row counts:\n",
    "- 68 races\n",
    "- 1,359 drivers\n",
    "- 74,605 laps\n",
    "\n",
    "üß¨ Integrity:\n",
    "- No orphan laps\n",
    "- Correct lap grain `(race_id, driver_code, lap_number)`\n",
    "\n",
    "üìÇ Track status:\n",
    "- Present for all races **on disk**\n",
    "- Ready for downstream use\n",
    "\n",
    "The database is now **analytically trustworthy**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß≠ How Our Understanding Changed\n",
    "\n",
    "### üîÅ Track Status Reframed\n",
    "Initially treated as a fact table ‚Äî now understood as:\n",
    "\n",
    "> A temporal overlay that must be mapped *after* lap timelines exist.\n",
    "\n",
    "### üß± PostgreSQL Repositioned\n",
    "Not a processing engine, but a **validated backbone**.\n",
    "\n",
    "### üß† Scope Tightened\n",
    "Notebook 03 ends **before any strategy logic** by design.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ NEXT STEPS ‚Äî Notebook 04 (VERY IMPORTANT)\n",
    "\n",
    "Notebook 04 is where **time finally enters the system**.\n",
    "\n",
    "### üìå Notebook 04 ‚Äî Cumulative Lap Timelines & Temporal Mapping\n",
    "\n",
    "#### 1Ô∏è‚É£ Compute cumulative race time per driver\n",
    "- Using `laps.lap_time_ms`\n",
    "- Ordered by `(race_id, driver_code, lap_number)`\n",
    "- Produces:\n",
    "  - `lap_start_time_ms`\n",
    "  - `lap_end_time_ms`\n",
    "\n",
    "This converts laps from **discrete rows** into a **continuous timeline**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Identify pit laps & out laps\n",
    "Using:\n",
    "- Lap time spikes\n",
    "- Tyre compound changes\n",
    "- Stint boundaries\n",
    "\n",
    "Outputs:\n",
    "- `is_pit_lap`\n",
    "- `is_out_lap`\n",
    "- `stint_id`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Load & map track status events üö¶\n",
    "- Read `track_status.parquet`\n",
    "- Convert events to time ranges\n",
    "- Overlay onto lap timelines\n",
    "\n",
    "Produces:\n",
    "- `is_green_lap`\n",
    "- `is_sc_lap`\n",
    "- `is_vsc_lap`\n",
    "\n",
    "This step is **impossible before Notebook 04**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ Filter analytically valid laps\n",
    "- Exclude:\n",
    "  - Safety Car laps\n",
    "  - Red flag laps\n",
    "  - Out laps\n",
    "- Preserve only:\n",
    "  - Competitive green-flag laps\n",
    "\n",
    "---\n",
    "\n",
    "#### 5Ô∏è‚É£ Prepare deterministic undercut inputs\n",
    "At the end of Notebook 04, we will have:\n",
    "\n",
    "- Comparable lap deltas\n",
    "- Clean stint transitions\n",
    "- Strategy-ready lap windows\n",
    "\n",
    "‚ö†Ô∏è **No undercut detection yet**\n",
    "That belongs to Notebook 05.\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ Final Reflection\n",
    "\n",
    "Notebook 03 transformed the project from:\n",
    "\n",
    "> ‚ÄúClean files on disk‚Äù\n",
    "\n",
    "into:\n",
    "\n",
    "> ‚ÄúA verified relational system that can survive scrutiny.‚Äù\n",
    "\n",
    "Every mistake exposed a hidden assumption.\n",
    "Every fix sharpened the architecture.\n",
    "\n",
    "With this foundation complete, **strategy analysis can now begin ‚Äî safely, reproducibly, and without shortcuts**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
