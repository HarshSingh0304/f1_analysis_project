{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a388486-0695-4707-9a2f-1a0be49c13ce",
   "metadata": {},
   "source": [
    "# üóÑÔ∏è Notebook 03 ‚Äî PostgreSQL Loading & Relational Validation\n",
    "\n",
    "## üéØ Objective of This Notebook\n",
    "\n",
    "Notebook 03 marks a **major architectural transition** in the project.\n",
    "\n",
    "Up to Notebook 02, all processing was performed in a **file-based analytical context** using standardized Parquet files. While that stage guaranteed **column-level correctness**, **unit consistency**, and **schema stability**, it deliberately avoided relational enforcement.\n",
    "\n",
    "This notebook exists to answer a different question:\n",
    "\n",
    "> ‚ÄúCan our standardized race data survive strict relational rules, repeated execution, and real analytical joins ‚Äî without breaking?‚Äù\n",
    "\n",
    "Notebook 03 is where the data pipeline becomes **structurally real**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Context from Notebook 02\n",
    "\n",
    "From Notebook 02, we now have:\n",
    "\n",
    "‚úÖ Fully standardized Parquet data  \n",
    "‚úÖ One directory per race  \n",
    "‚úÖ One canonical schema for each dataset  \n",
    "‚úÖ No ambiguity in column names or units  \n",
    "\n",
    "Directory structure:\n",
    "```\n",
    "data/interim/standardized/\n",
    "‚îî‚îÄ‚îÄ year=YYYY/\n",
    "    ‚îî‚îÄ‚îÄ round=XX_<race_name>/\n",
    "        ‚îú‚îÄ‚îÄ laps.parquet\n",
    "        ‚îú‚îÄ‚îÄ results.parquet\n",
    "        ‚îî‚îÄ‚îÄ track_status.parquet\n",
    "```\n",
    "Important guarantees from Notebook 02:\n",
    "\n",
    "- üü¢ Lap times are in **milliseconds**\n",
    "- üü¢ Lap grain is **one row per driver per lap**\n",
    "- üü¢ Driver identifiers are stable *within a race*\n",
    "- üü¢ No downstream notebook is allowed to ‚Äúfix‚Äù schema issues\n",
    "\n",
    "Notebook 03 **must adapt to this data**, not reshape it.\n",
    "\n",
    "---\n",
    "\n",
    "## üêò Why PostgreSQL Is Introduced Here\n",
    "\n",
    "PostgreSQL is introduced **only after standardization**, not before, because:\n",
    "\n",
    "- üì¶ Parquet cannot enforce:\n",
    "  - Primary keys\n",
    "  - Foreign keys\n",
    "  - Idempotent writes\n",
    "- üîó Strategy analysis requires **trustworthy joins**\n",
    "- üîÅ Re-runnable pipelines require **conflict handling**\n",
    "\n",
    "PostgreSQL in this project is:\n",
    "\n",
    "‚úÖ An analytical backbone  \n",
    "‚ùå Not a transformation engine  \n",
    "‚ùå Not a cleaning layer  \n",
    "\n",
    "---\n",
    "\n",
    "## üß± What This Notebook Will Do\n",
    "\n",
    "This notebook will:\n",
    "\n",
    "### 1Ô∏è‚É£ Establish a PostgreSQL connection\n",
    "- Using environment-based configuration\n",
    "- Fully portable across machines\n",
    "\n",
    "### 2Ô∏è‚É£ Define strict relational schema\n",
    "Including:\n",
    "\n",
    "- **races**\n",
    "  - One row per race\n",
    "  - Identified by `race_id`\n",
    "- **drivers**\n",
    "  - One row per `(race_id, driver_code)`\n",
    "- **laps**\n",
    "  - One row per `(race_id, driver_code, lap_number)`\n",
    "\n",
    "### 3Ô∏è‚É£ Enforce explicit data grain\n",
    "- No implicit joins\n",
    "- No inferred keys\n",
    "- No tolerance for duplicates\n",
    "\n",
    "### 4Ô∏è‚É£ Load standardized Parquet ‚Üí PostgreSQL\n",
    "- One race at a time\n",
    "- With defensive column selection\n",
    "- Ignoring irrelevant columns safely\n",
    "\n",
    "### 5Ô∏è‚É£ Guarantee idempotency üîÅ\n",
    "- Safe kernel restarts\n",
    "- Safe re-execution\n",
    "- No duplicate inserts\n",
    "- No manual truncation required\n",
    "\n",
    "### 6Ô∏è‚É£ Validate relational integrity üß™\n",
    "- Row counts\n",
    "- Orphan detection\n",
    "- Lap grain uniqueness\n",
    "\n",
    "---\n",
    "\n",
    "## üö´ What This Notebook Will NOT Do\n",
    "\n",
    "This notebook explicitly does **not**:\n",
    "\n",
    "‚ùå Compute cumulative lap times  \n",
    "‚ùå Identify pit laps or out laps  \n",
    "‚ùå Detect undercuts  \n",
    "‚ùå Perform any race strategy logic  \n",
    "‚ùå Load track status into PostgreSQL  \n",
    "\n",
    "Those steps require **derived temporal context** and belong downstream.\n",
    "\n",
    "---\n",
    "\n",
    "## üö¶ Track Status: Intentional Deferral\n",
    "\n",
    "Although `track_status.parquet` exists for every race:\n",
    "\n",
    "- It is **event-based**, not lap-based\n",
    "- It cannot be joined meaningfully without lap timelines\n",
    "- Loading it prematurely caused grain violations during development\n",
    "\n",
    "‚û°Ô∏è Therefore:\n",
    "- Track status files are **validated for presence**\n",
    "- Actual usage is deferred to a later notebook\n",
    "\n",
    "This is a **design decision**, not a limitation.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Expected Outcome of Notebook 03\n",
    "\n",
    "By the end of this notebook, we expect:\n",
    "\n",
    "‚úî PostgreSQL populated with all races (2022‚Äì2024)  \n",
    "‚úî Strict relational invariants enforced  \n",
    "‚úî Fully idempotent execution  \n",
    "‚úî Database safe for analytical feature computation  \n",
    "\n",
    "Notebook 03 does **not produce insights** ‚Äî it produces **trust**.\n",
    "\n",
    "Everything that follows depends on this foundation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "967607ab-493a-45c0-b974-a8f7e06dd75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 10:46:04,116 | INFO | src.logging_config | Starting Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
      "2025-12-31 10:46:04,119 | INFO | src.logging_config | Database environment variables loaded successfully\n",
      "2025-12-31 10:46:04,221 | INFO | src.logging_config | PostgreSQL engine constructed\n",
      "2025-12-31 10:46:04,236 | INFO | src.logging_config | Discovered 408 standardized Parquet files\n",
      "2025-12-31 10:46:04,300 | INFO | src.logging_config | Existing tables in database: ['track_status', 'laps', 'drivers', 'races', 'lap_features', 'undercut_events', 'undercut_summary', 'results', 'weather_data', 'session_status', 'race_control_messages']\n",
      "\n",
      "============================================================\n",
      "Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
      "------------------------------------------------------------\n",
      "This notebook will:\n",
      "  - Load ALL standardized Parquet tables into PostgreSQL\n",
      "    (laps, results, track_status, weather_data,\n",
      "     session_status, race_control_messages)\n",
      "  - Define strict relational schemas and primary keys\n",
      "  - Enforce explicit data grain and idempotent writes\n",
      "  - Materialize a relational foundation for downstream analysis\n",
      "\n",
      "This notebook will not:\n",
      "  - Call external APIs\n",
      "  - Perform semantic or column-level standardization\n",
      "  - Detect undercut events\n",
      "  - Produce analytical conclusions\n",
      "\n",
      "Notebook 02 outputs are treated as authoritative.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 1: Environment setup and execution preconditions\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Resolve project root dynamically (Jupyter-safe)\n",
    "# ------------------------------------------------------------\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_project_root(start_path: Path) -> Path:\n",
    "    current = start_path.resolve()\n",
    "    for parent in [current] + list(current.parents):\n",
    "        if (parent / \"src\").exists():\n",
    "            return parent\n",
    "    raise RuntimeError(\n",
    "        \"Could not locate project root. Ensure notebook is inside the project.\"\n",
    "    )\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Standard library and third-party imports\n",
    "# ------------------------------------------------------------\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine, inspect\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Project imports\n",
    "# ------------------------------------------------------------\n",
    "from src.config import Config\n",
    "from src.logging_config import setup_logging\n",
    "from src.utils import ensure_dir\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Initialize logging\n",
    "# ------------------------------------------------------------\n",
    "logger, error_logger = setup_logging()\n",
    "logger.info(\"Starting Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load environment variables (.env)\n",
    "# ------------------------------------------------------------\n",
    "load_dotenv(PROJECT_ROOT / \".env\")\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "missing_env = [\n",
    "    var for var in [\n",
    "        \"DB_USER\", \"DB_PASSWORD\", \"DB_HOST\", \"DB_PORT\", \"DB_NAME\"\n",
    "    ] if os.getenv(var) is None\n",
    "]\n",
    "\n",
    "if missing_env:\n",
    "    raise EnvironmentError(\n",
    "        f\"Missing required database environment variables: {missing_env}\"\n",
    "    )\n",
    "\n",
    "logger.info(\"Database environment variables loaded successfully\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create PostgreSQL engine (no connection yet)\n",
    "# ------------------------------------------------------------\n",
    "DATABASE_URL = (\n",
    "    f\"postgresql://{DB_USER}:{DB_PASSWORD}\"\n",
    "    f\"@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "logger.info(\"PostgreSQL engine constructed\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Resolve standardized Parquet input directory\n",
    "# ------------------------------------------------------------\n",
    "STANDARDIZED_DATA_DIR = Config.DATA_DIR / \"interim\" / \"standardized\"\n",
    "\n",
    "if not STANDARDIZED_DATA_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Standardized Parquet directory does not exist. \"\n",
    "        \"Notebook 02 must be executed successfully before Notebook 03.\"\n",
    "    )\n",
    "\n",
    "standardized_files = list(STANDARDIZED_DATA_DIR.rglob(\"*.parquet\"))\n",
    "\n",
    "if not standardized_files:\n",
    "    raise FileNotFoundError(\n",
    "        \"No standardized Parquet files found. \"\n",
    "        \"Notebook 02 did not produce usable outputs.\"\n",
    "    )\n",
    "\n",
    "logger.info(\n",
    "    f\"Discovered {len(standardized_files)} standardized Parquet files\"\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Inspect database state (read-only)\n",
    "# ------------------------------------------------------------\n",
    "inspector = inspect(engine)\n",
    "existing_tables = inspector.get_table_names()\n",
    "\n",
    "logger.info(f\"Existing tables in database: {existing_tables}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Declare execution scope (UPDATED ‚Äî scope-aligned)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\"\"\n",
    "============================================================\n",
    "Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "------------------------------------------------------------\n",
    "This notebook will:\n",
    "  - Load ALL standardized Parquet tables into PostgreSQL\n",
    "    (laps, results, track_status, weather_data,\n",
    "     session_status, race_control_messages)\n",
    "  - Define strict relational schemas and primary keys\n",
    "  - Enforce explicit data grain and idempotent writes\n",
    "  - Materialize a relational foundation for downstream analysis\n",
    "\n",
    "This notebook will not:\n",
    "  - Call external APIs\n",
    "  - Perform semantic or column-level standardization\n",
    "  - Detect undercut events\n",
    "  - Produce analytical conclusions\n",
    "\n",
    "Notebook 02 outputs are treated as authoritative.\n",
    "============================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa1716b-ba9e-4baa-98b5-19a406c409fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 10:46:04,326 | INFO | src.logging_config | Defining relational schema from standardized Parquet (no derivations)\n",
      "2025-12-31 10:46:04,382 | INFO | src.logging_config | Relational schema defined for ALL standardized Parquet tables (no transformations, no derivations)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 2: Relational schema definition (FULL standardized scope)\n",
    "# ============================================================\n",
    "\n",
    "from sqlalchemy import (\n",
    "    Table, Column, Integer, BigInteger, String,\n",
    "    Boolean, MetaData, ForeignKey, ForeignKeyConstraint,\n",
    "    PrimaryKeyConstraint, Index, CheckConstraint\n",
    ")\n",
    "\n",
    "logger.info(\"Defining relational schema from standardized Parquet (no derivations)\")\n",
    "\n",
    "metadata = MetaData()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RACES\n",
    "# One row per race\n",
    "# race_id = \"{season}_{round}\"\n",
    "# ------------------------------------------------------------\n",
    "races = Table(\n",
    "    \"races\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, primary_key=True),\n",
    "    Column(\"season\", Integer, nullable=False),\n",
    "    Column(\"round\", Integer, nullable=False),\n",
    "\n",
    "    CheckConstraint(\"season >= 1950\", name=\"ck_races_valid_season\"),\n",
    "    CheckConstraint(\"round > 0\", name=\"ck_races_valid_round\"),\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# DRIVERS\n",
    "# One row per driver per race\n",
    "# Identity is race-scoped\n",
    "# ------------------------------------------------------------\n",
    "drivers = Table(\n",
    "    \"drivers\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, ForeignKey(\"races.race_id\"), nullable=False),\n",
    "    Column(\"driver_code\", String, nullable=False),\n",
    "    Column(\"driver_number\", Integer, nullable=False),\n",
    "\n",
    "    PrimaryKeyConstraint(\n",
    "        \"race_id\",\n",
    "        \"driver_code\",\n",
    "        name=\"pk_drivers\"\n",
    "    ),\n",
    "\n",
    "    CheckConstraint(\n",
    "        \"driver_number > 0\",\n",
    "        name=\"ck_drivers_valid_number\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "Index(\"idx_drivers_race\", drivers.c.race_id)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# LAPS\n",
    "# One row per (race_id, driver_code, lap_number)\n",
    "# Column names EXACTLY as in standardized Parquet\n",
    "# ------------------------------------------------------------\n",
    "laps = Table(\n",
    "    \"laps\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, ForeignKey(\"races.race_id\"), nullable=False),\n",
    "    Column(\"driver_code\", String, nullable=False),\n",
    "    Column(\"lap_number\", Integer, nullable=False),\n",
    "\n",
    "    # Standardized columns (no renaming)\n",
    "    Column(\"LapTime\", BigInteger, nullable=True),\n",
    "    Column(\"Compound\", String, nullable=True),\n",
    "\n",
    "    PrimaryKeyConstraint(\n",
    "        \"race_id\",\n",
    "        \"driver_code\",\n",
    "        \"lap_number\",\n",
    "        name=\"pk_laps\"\n",
    "    ),\n",
    "\n",
    "    ForeignKeyConstraint(\n",
    "        [\"race_id\", \"driver_code\"],\n",
    "        [\"drivers.race_id\", \"drivers.driver_code\"],\n",
    "        name=\"fk_laps_drivers\"\n",
    "    ),\n",
    "\n",
    "    CheckConstraint(\"lap_number > 0\", name=\"ck_laps_valid_lap\"),\n",
    ")\n",
    "\n",
    "Index(\"idx_laps_race_driver\", laps.c.race_id, laps.c.driver_code)\n",
    "Index(\"idx_laps_race_lap\", laps.c.race_id, laps.c.lap_number)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RESULTS\n",
    "# One row per driver per race\n",
    "# Mirrors standardized results.parquet\n",
    "# ------------------------------------------------------------\n",
    "results = Table(\n",
    "    \"results\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, ForeignKey(\"races.race_id\"), nullable=False),\n",
    "    Column(\"driver_code\", String, nullable=False),\n",
    "\n",
    "    Column(\"position\", Integer, nullable=True),\n",
    "    Column(\"points\", Integer, nullable=True),\n",
    "    Column(\"status\", String, nullable=True),\n",
    "\n",
    "    PrimaryKeyConstraint(\n",
    "        \"race_id\",\n",
    "        \"driver_code\",\n",
    "        name=\"pk_results\"\n",
    "    ),\n",
    "\n",
    "    ForeignKeyConstraint(\n",
    "        [\"race_id\", \"driver_code\"],\n",
    "        [\"drivers.race_id\", \"drivers.driver_code\"],\n",
    "        name=\"fk_results_drivers\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "Index(\"idx_results_race\", results.c.race_id)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# TRACK STATUS\n",
    "# Event-based table (NO lap mapping here)\n",
    "# ------------------------------------------------------------\n",
    "track_status = Table(\n",
    "    \"track_status\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, ForeignKey(\"races.race_id\"), nullable=False),\n",
    "\n",
    "    # Event-time ordering preserved from Parquet\n",
    "    Column(\"Time\", BigInteger, nullable=False),\n",
    "    Column(\"Status\", String, nullable=False),\n",
    "\n",
    "    PrimaryKeyConstraint(\n",
    "        \"race_id\",\n",
    "        \"Time\",\n",
    "        name=\"pk_track_status\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "Index(\"idx_track_status_race_time\", track_status.c.race_id, track_status.c.Time)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# WEATHER DATA\n",
    "# Time-based environmental observations\n",
    "# ------------------------------------------------------------\n",
    "weather_data = Table(\n",
    "    \"weather_data\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, ForeignKey(\"races.race_id\"), nullable=False),\n",
    "\n",
    "    Column(\"Time\", BigInteger, nullable=False),\n",
    "    Column(\"AirTemp\", Integer, nullable=True),\n",
    "    Column(\"TrackTemp\", Integer, nullable=True),\n",
    "    Column(\"Humidity\", Integer, nullable=True),\n",
    "    Column(\"Pressure\", Integer, nullable=True),\n",
    "    Column(\"WindSpeed\", Integer, nullable=True),\n",
    "    Column(\"WindDirection\", Integer, nullable=True),\n",
    "\n",
    "    PrimaryKeyConstraint(\n",
    "        \"race_id\",\n",
    "        \"Time\",\n",
    "        name=\"pk_weather_data\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "Index(\"idx_weather_race_time\", weather_data.c.race_id, weather_data.c.Time)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SESSION STATUS\n",
    "# Session-level lifecycle events\n",
    "# ------------------------------------------------------------\n",
    "session_status = Table(\n",
    "    \"session_status\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, ForeignKey(\"races.race_id\"), nullable=False),\n",
    "\n",
    "    Column(\"Time\", BigInteger, nullable=False),\n",
    "    Column(\"Status\", String, nullable=False),\n",
    "\n",
    "    PrimaryKeyConstraint(\n",
    "        \"race_id\",\n",
    "        \"Time\",\n",
    "        name=\"pk_session_status\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "Index(\"idx_session_status_race_time\", session_status.c.race_id, session_status.c.Time)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# RACE CONTROL MESSAGES\n",
    "# Event-based race control messages\n",
    "# ------------------------------------------------------------\n",
    "race_control_messages = Table(\n",
    "    \"race_control_messages\",\n",
    "    metadata,\n",
    "    Column(\"race_id\", String, ForeignKey(\"races.race_id\"), nullable=False),\n",
    "\n",
    "    Column(\"Time\", BigInteger, nullable=False),\n",
    "    Column(\"Category\", String, nullable=True),\n",
    "    Column(\"Message\", String, nullable=True),\n",
    "\n",
    "    PrimaryKeyConstraint(\n",
    "        \"race_id\",\n",
    "        \"Time\",\n",
    "        name=\"pk_race_control_messages\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "Index(\n",
    "    \"idx_rcm_race_time\",\n",
    "    race_control_messages.c.race_id,\n",
    "    race_control_messages.c.Time\n",
    ")\n",
    "\n",
    "logger.info(\n",
    "    \"Relational schema defined for ALL standardized Parquet tables \"\n",
    "    \"(no transformations, no derivations)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "852792d8-2817-4941-a444-7488ffc5e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 10:46:04,396 | INFO | src.logging_config | Ensuring PostgreSQL schema exists for Notebook 03 tables\n",
      "2025-12-31 10:46:04,401 | INFO | src.logging_config | All required tables already exist ‚Äî skipping DDL execution\n",
      "2025-12-31 10:46:04,404 | INFO | src.logging_config | Verified required tables present: ['drivers', 'laps', 'race_control_messages', 'races', 'results', 'session_status', 'track_status', 'weather_data']\n",
      "\n",
      "============================================================\n",
      "Notebook 03 ‚Äî Schema Ready\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ Required relational tables exist\n",
      "‚Ä¢ Schema creation is idempotent\n",
      "‚Ä¢ Compatible with future derived tables\n",
      "‚Ä¢ Safe across kernel restarts\n",
      "\n",
      "This notebook owns ONLY its required tables.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 3: Execute DDL (robust, idempotent, future-safe)\n",
    "# ============================================================\n",
    "\n",
    "from sqlalchemy import inspect\n",
    "\n",
    "logger.info(\"Ensuring PostgreSQL schema exists for Notebook 03 tables\")\n",
    "\n",
    "inspector = inspect(engine)\n",
    "existing_tables = set(inspector.get_table_names())\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Tables that THIS notebook owns and is responsible for\n",
    "# (must exist after this cell)\n",
    "# ------------------------------------------------------------\n",
    "required_tables = {\n",
    "    \"races\",\n",
    "    \"drivers\",\n",
    "    \"laps\",\n",
    "    \"results\",\n",
    "    \"track_status\",\n",
    "    \"weather_data\",\n",
    "    \"session_status\",\n",
    "    \"race_control_messages\",\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Determine which required tables are missing\n",
    "# ------------------------------------------------------------\n",
    "missing_tables = required_tables - existing_tables\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Create only missing tables (idempotent)\n",
    "# ------------------------------------------------------------\n",
    "if missing_tables:\n",
    "    logger.info(\n",
    "        f\"Missing required tables detected: {sorted(missing_tables)}. \"\n",
    "        \"Creating missing tables only.\"\n",
    "    )\n",
    "\n",
    "    metadata.create_all(\n",
    "        engine,\n",
    "        tables=[\n",
    "            table\n",
    "            for table in metadata.tables.values()\n",
    "            if table.name in missing_tables\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    logger.info(\"Missing tables created successfully\")\n",
    "\n",
    "else:\n",
    "    logger.info(\n",
    "        \"All required tables already exist ‚Äî skipping DDL execution\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Final verification (subset invariant)\n",
    "# ------------------------------------------------------------\n",
    "# Re-inspect database AFTER DDL (inspector is not live)\n",
    "final_inspector = inspect(engine)\n",
    "final_tables = set(final_inspector.get_table_names())\n",
    "still_missing = required_tables - final_tables\n",
    "\n",
    "if still_missing:\n",
    "    raise RuntimeError(\n",
    "        f\"Schema verification failed. Missing required tables: \"\n",
    "        f\"{sorted(still_missing)}\"\n",
    "    )\n",
    "\n",
    "logger.info(\n",
    "    f\"Verified required tables present: {sorted(required_tables)}\"\n",
    ")\n",
    "\n",
    "print(\"\"\"\n",
    "============================================================\n",
    "Notebook 03 ‚Äî Schema Ready\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ Required relational tables exist\n",
    "‚Ä¢ Schema creation is idempotent\n",
    "‚Ä¢ Compatible with future derived tables\n",
    "‚Ä¢ Safe across kernel restarts\n",
    "\n",
    "This notebook owns ONLY its required tables.\n",
    "============================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1feee197-3eb1-4021-8dc5-1e3e2aa7aeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 10:46:04,414 | INFO | src.logging_config | Starting standardized Parquet schema diagnostics\n",
      "2025-12-31 10:46:06,733 | INFO | src.logging_config | Inspected 408 standardized Parquet files\n",
      "2025-12-31 10:46:06,770 | INFO | src.logging_config | Schema snapshot written to C:\\Users\\hersh\\Desktop\\f1_analysis_project\\data\\interim\\standardized_schema_snapshot.json\n",
      "\n",
      "============================================================\n",
      "Notebook 03 ‚Äî Standardized Parquet Diagnostics Complete\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ All standardized Parquet files inspected\n",
      "‚Ä¢ Exact schemas captured per table per race\n",
      "‚Ä¢ No assumptions made about race identity\n",
      "‚Ä¢ Schema snapshot persisted for loader design\n",
      "\n",
      "Next step:\n",
      "  - Inspect standardized_schema_snapshot.json\n",
      "  - Decide authoritative race identity source\n",
      "  - Design robust loading logic (Cell 5)\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 4: Standardized Parquet Schema Diagnostics\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "from src.config import Config\n",
    "\n",
    "logger.info(\"Starting standardized Parquet schema diagnostics\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Resolve paths using authoritative Config\n",
    "# ------------------------------------------------------------\n",
    "INTERIM_DATA_DIR = Config.DATA_DIR / \"interim\"\n",
    "STANDARDIZED_SCHEMA_DIR = INTERIM_DATA_DIR / \"standardized\"\n",
    "SCHEMA_OUTPUT_PATH = INTERIM_DATA_DIR / \"standardized_schema_snapshot.json\"\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Validate directory exists\n",
    "# ------------------------------------------------------------\n",
    "if not STANDARDIZED_SCHEMA_DIR.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Standardized data directory not found: {STANDARDIZED_SCHEMA_DIR}\"\n",
    "    )\n",
    "\n",
    "schema_snapshot = defaultdict(dict)\n",
    "file_count = 0\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Inspect all standardized parquet files\n",
    "# ------------------------------------------------------------\n",
    "for parquet_path in STANDARDIZED_SCHEMA_DIR.rglob(\"*.parquet\"):\n",
    "    race_dir = parquet_path.parent\n",
    "    table_name = parquet_path.stem\n",
    "\n",
    "    season_folder = race_dir.parts[-2]   # e.g. year=2024\n",
    "    race_folder = race_dir.name           # e.g. round=10_Spanish_Grand_Prix\n",
    "    race_key = f\"{season_folder}/{race_folder}\"\n",
    "\n",
    "    df = pd.read_parquet(parquet_path)\n",
    "\n",
    "    schema_snapshot.setdefault(table_name, {})\n",
    "    schema_snapshot[table_name][race_key] = {\n",
    "        \"columns\": list(df.columns),\n",
    "        \"dtypes\": {col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "        \"row_count\": int(len(df)),\n",
    "    }\n",
    "\n",
    "    file_count += 1\n",
    "\n",
    "logger.info(f\"Inspected {file_count} standardized Parquet files\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Persist schema snapshot\n",
    "# ------------------------------------------------------------\n",
    "with open(SCHEMA_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema_snapshot, f, indent=2)\n",
    "\n",
    "logger.info(f\"Schema snapshot written to {SCHEMA_OUTPUT_PATH}\")\n",
    "\n",
    "print(\"\"\"\n",
    "============================================================\n",
    "Notebook 03 ‚Äî Standardized Parquet Diagnostics Complete\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ All standardized Parquet files inspected\n",
    "‚Ä¢ Exact schemas captured per table per race\n",
    "‚Ä¢ No assumptions made about race identity\n",
    "‚Ä¢ Schema snapshot persisted for loader design\n",
    "\n",
    "Next step:\n",
    "  - Inspect standardized_schema_snapshot.json\n",
    "  - Decide authoritative race identity source\n",
    "  - Design robust loading logic (Cell 5)\n",
    "============================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f85dd27-e9a9-4559-b2ac-52644c00ba21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 10:46:06,792 | INFO | src.logging_config | Starting standardized Parquet ‚Üí PostgreSQL load (idempotent)\n",
      "2025-12-31 10:46:06,796 | INFO | src.logging_config | Processing race 2022_10\n",
      "2025-12-31 10:46:06,799 | INFO | src.logging_config | Race 2022_10 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,800 | INFO | src.logging_config | Processing race 2022_11\n",
      "2025-12-31 10:46:06,801 | INFO | src.logging_config | Race 2022_11 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,802 | INFO | src.logging_config | Processing race 2022_12\n",
      "2025-12-31 10:46:06,804 | INFO | src.logging_config | Race 2022_12 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,806 | INFO | src.logging_config | Processing race 2022_13\n",
      "2025-12-31 10:46:06,807 | INFO | src.logging_config | Race 2022_13 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,808 | INFO | src.logging_config | Processing race 2022_14\n",
      "2025-12-31 10:46:06,810 | INFO | src.logging_config | Race 2022_14 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,811 | INFO | src.logging_config | Processing race 2022_15\n",
      "2025-12-31 10:46:06,812 | INFO | src.logging_config | Race 2022_15 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,813 | INFO | src.logging_config | Processing race 2022_16\n",
      "2025-12-31 10:46:06,815 | INFO | src.logging_config | Race 2022_16 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,816 | INFO | src.logging_config | Processing race 2022_17\n",
      "2025-12-31 10:46:06,817 | INFO | src.logging_config | Race 2022_17 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,818 | INFO | src.logging_config | Processing race 2022_18\n",
      "2025-12-31 10:46:06,820 | INFO | src.logging_config | Race 2022_18 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,820 | INFO | src.logging_config | Processing race 2022_19\n",
      "2025-12-31 10:46:06,822 | INFO | src.logging_config | Race 2022_19 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,823 | INFO | src.logging_config | Processing race 2022_1\n",
      "2025-12-31 10:46:06,824 | INFO | src.logging_config | Race 2022_1 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,825 | INFO | src.logging_config | Processing race 2022_20\n",
      "2025-12-31 10:46:06,826 | INFO | src.logging_config | Race 2022_20 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,827 | INFO | src.logging_config | Processing race 2022_21\n",
      "2025-12-31 10:46:06,828 | INFO | src.logging_config | Race 2022_21 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,829 | INFO | src.logging_config | Processing race 2022_22\n",
      "2025-12-31 10:46:06,830 | INFO | src.logging_config | Race 2022_22 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,830 | INFO | src.logging_config | Processing race 2022_2\n",
      "2025-12-31 10:46:06,831 | INFO | src.logging_config | Race 2022_2 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,832 | INFO | src.logging_config | Processing race 2022_3\n",
      "2025-12-31 10:46:06,834 | INFO | src.logging_config | Race 2022_3 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,834 | INFO | src.logging_config | Processing race 2022_4\n",
      "2025-12-31 10:46:06,836 | INFO | src.logging_config | Race 2022_4 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,837 | INFO | src.logging_config | Processing race 2022_5\n",
      "2025-12-31 10:46:06,838 | INFO | src.logging_config | Race 2022_5 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,839 | INFO | src.logging_config | Processing race 2022_6\n",
      "2025-12-31 10:46:06,841 | INFO | src.logging_config | Race 2022_6 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,842 | INFO | src.logging_config | Processing race 2022_7\n",
      "2025-12-31 10:46:06,843 | INFO | src.logging_config | Race 2022_7 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,844 | INFO | src.logging_config | Processing race 2022_8\n",
      "2025-12-31 10:46:06,845 | INFO | src.logging_config | Race 2022_8 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,846 | INFO | src.logging_config | Processing race 2022_9\n",
      "2025-12-31 10:46:06,847 | INFO | src.logging_config | Race 2022_9 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,848 | INFO | src.logging_config | Processing race 2023_10\n",
      "2025-12-31 10:46:06,849 | INFO | src.logging_config | Race 2023_10 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,850 | INFO | src.logging_config | Processing race 2023_11\n",
      "2025-12-31 10:46:06,851 | INFO | src.logging_config | Race 2023_11 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,852 | INFO | src.logging_config | Processing race 2023_12\n",
      "2025-12-31 10:46:06,853 | INFO | src.logging_config | Race 2023_12 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,854 | INFO | src.logging_config | Processing race 2023_13\n",
      "2025-12-31 10:46:06,855 | INFO | src.logging_config | Race 2023_13 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,856 | INFO | src.logging_config | Processing race 2023_14\n",
      "2025-12-31 10:46:06,858 | INFO | src.logging_config | Race 2023_14 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,859 | INFO | src.logging_config | Processing race 2023_15\n",
      "2025-12-31 10:46:06,860 | INFO | src.logging_config | Race 2023_15 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,860 | INFO | src.logging_config | Processing race 2023_16\n",
      "2025-12-31 10:46:06,862 | INFO | src.logging_config | Race 2023_16 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,862 | INFO | src.logging_config | Processing race 2023_17\n",
      "2025-12-31 10:46:06,863 | INFO | src.logging_config | Race 2023_17 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,864 | INFO | src.logging_config | Processing race 2023_18\n",
      "2025-12-31 10:46:06,865 | INFO | src.logging_config | Race 2023_18 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,866 | INFO | src.logging_config | Processing race 2023_19\n",
      "2025-12-31 10:46:06,867 | INFO | src.logging_config | Race 2023_19 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,868 | INFO | src.logging_config | Processing race 2023_1\n",
      "2025-12-31 10:46:06,870 | INFO | src.logging_config | Race 2023_1 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,871 | INFO | src.logging_config | Processing race 2023_20\n",
      "2025-12-31 10:46:06,872 | INFO | src.logging_config | Race 2023_20 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,873 | INFO | src.logging_config | Processing race 2023_21\n",
      "2025-12-31 10:46:06,875 | INFO | src.logging_config | Race 2023_21 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,875 | INFO | src.logging_config | Processing race 2023_22\n",
      "2025-12-31 10:46:06,876 | INFO | src.logging_config | Race 2023_22 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,877 | INFO | src.logging_config | Processing race 2023_2\n",
      "2025-12-31 10:46:06,879 | INFO | src.logging_config | Race 2023_2 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,879 | INFO | src.logging_config | Processing race 2023_3\n",
      "2025-12-31 10:46:06,881 | INFO | src.logging_config | Race 2023_3 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,881 | INFO | src.logging_config | Processing race 2023_4\n",
      "2025-12-31 10:46:06,883 | INFO | src.logging_config | Race 2023_4 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,883 | INFO | src.logging_config | Processing race 2023_5\n",
      "2025-12-31 10:46:06,884 | INFO | src.logging_config | Race 2023_5 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,885 | INFO | src.logging_config | Processing race 2023_6\n",
      "2025-12-31 10:46:06,886 | INFO | src.logging_config | Race 2023_6 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,888 | INFO | src.logging_config | Processing race 2023_7\n",
      "2025-12-31 10:46:06,889 | INFO | src.logging_config | Race 2023_7 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,890 | INFO | src.logging_config | Processing race 2023_8\n",
      "2025-12-31 10:46:06,891 | INFO | src.logging_config | Race 2023_8 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,892 | INFO | src.logging_config | Processing race 2023_9\n",
      "2025-12-31 10:46:06,893 | INFO | src.logging_config | Race 2023_9 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,895 | INFO | src.logging_config | Processing race 2024_10\n",
      "2025-12-31 10:46:06,896 | INFO | src.logging_config | Race 2024_10 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,896 | INFO | src.logging_config | Processing race 2024_11\n",
      "2025-12-31 10:46:06,897 | INFO | src.logging_config | Race 2024_11 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,898 | INFO | src.logging_config | Processing race 2024_12\n",
      "2025-12-31 10:46:06,899 | INFO | src.logging_config | Race 2024_12 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,900 | INFO | src.logging_config | Processing race 2024_13\n",
      "2025-12-31 10:46:06,901 | INFO | src.logging_config | Race 2024_13 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,902 | INFO | src.logging_config | Processing race 2024_14\n",
      "2025-12-31 10:46:06,904 | INFO | src.logging_config | Race 2024_14 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,905 | INFO | src.logging_config | Processing race 2024_15\n",
      "2025-12-31 10:46:06,906 | INFO | src.logging_config | Race 2024_15 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,907 | INFO | src.logging_config | Processing race 2024_16\n",
      "2025-12-31 10:46:06,908 | INFO | src.logging_config | Race 2024_16 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,909 | INFO | src.logging_config | Processing race 2024_17\n",
      "2025-12-31 10:46:06,911 | INFO | src.logging_config | Race 2024_17 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,911 | INFO | src.logging_config | Processing race 2024_18\n",
      "2025-12-31 10:46:06,912 | INFO | src.logging_config | Race 2024_18 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,913 | INFO | src.logging_config | Processing race 2024_19\n",
      "2025-12-31 10:46:06,914 | INFO | src.logging_config | Race 2024_19 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,915 | INFO | src.logging_config | Processing race 2024_1\n",
      "2025-12-31 10:46:06,916 | INFO | src.logging_config | Race 2024_1 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,917 | INFO | src.logging_config | Processing race 2024_20\n",
      "2025-12-31 10:46:06,918 | INFO | src.logging_config | Race 2024_20 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,919 | INFO | src.logging_config | Processing race 2024_21\n",
      "2025-12-31 10:46:06,920 | INFO | src.logging_config | Race 2024_21 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,921 | INFO | src.logging_config | Processing race 2024_22\n",
      "2025-12-31 10:46:06,923 | INFO | src.logging_config | Race 2024_22 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,923 | INFO | src.logging_config | Processing race 2024_23\n",
      "2025-12-31 10:46:06,925 | INFO | src.logging_config | Race 2024_23 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,926 | INFO | src.logging_config | Processing race 2024_24\n",
      "2025-12-31 10:46:06,927 | INFO | src.logging_config | Race 2024_24 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,928 | INFO | src.logging_config | Processing race 2024_2\n",
      "2025-12-31 10:46:06,929 | INFO | src.logging_config | Race 2024_2 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,930 | INFO | src.logging_config | Processing race 2024_3\n",
      "2025-12-31 10:46:06,932 | INFO | src.logging_config | Race 2024_3 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,933 | INFO | src.logging_config | Processing race 2024_4\n",
      "2025-12-31 10:46:06,936 | INFO | src.logging_config | Race 2024_4 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,937 | INFO | src.logging_config | Processing race 2024_5\n",
      "2025-12-31 10:46:06,939 | INFO | src.logging_config | Race 2024_5 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,940 | INFO | src.logging_config | Processing race 2024_6\n",
      "2025-12-31 10:46:06,941 | INFO | src.logging_config | Race 2024_6 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,942 | INFO | src.logging_config | Processing race 2024_7\n",
      "2025-12-31 10:46:06,944 | INFO | src.logging_config | Race 2024_7 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,945 | INFO | src.logging_config | Processing race 2024_8\n",
      "2025-12-31 10:46:06,946 | INFO | src.logging_config | Race 2024_8 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,947 | INFO | src.logging_config | Processing race 2024_9\n",
      "2025-12-31 10:46:06,948 | INFO | src.logging_config | Race 2024_9 already loaded ‚Äî skipping\n",
      "2025-12-31 10:46:06,949 | INFO | src.logging_config | Starting standardized Parquet -> PostgreSQL load (idempotent)\n",
      "\n",
      "============================================================\n",
      "Notebook 03 ‚Äî Data Load Complete (Idempotent)\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ Existing races safely skipped\n",
      "‚Ä¢ All six standardized tables loaded\n",
      "‚Ä¢ Column names preserved exactly\n",
      "‚Ä¢ Relational invariants respected\n",
      "\n",
      "Notebook 03 is now structurally complete.\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 5: Idempotent Standardized Parquet ‚Üí PostgreSQL Load\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import text\n",
    "from src.config import Config\n",
    "\n",
    "logger.info(\"Starting standardized Parquet ‚Üí PostgreSQL load (idempotent)\")\n",
    "\n",
    "STANDARDIZED_DIR = Config.DATA_DIR / \"interim\" / \"standardized\"\n",
    "\n",
    "with engine.begin() as conn:\n",
    "\n",
    "    for year_dir in sorted(STANDARDIZED_DIR.glob(\"year=*\")):\n",
    "        season = int(year_dir.name.split(\"=\")[1])\n",
    "\n",
    "        for race_dir in sorted(year_dir.iterdir()):\n",
    "            round_number = int(race_dir.name.split(\"_\")[0].split(\"=\")[1])\n",
    "            race_id = f\"{season}_{round_number}\"\n",
    "\n",
    "            logger.info(f\"Processing race {race_id}\")\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # Skip race if already loaded (authoritative gate)\n",
    "            # ------------------------------------------------\n",
    "            race_exists = conn.execute(\n",
    "                text(\"SELECT 1 FROM races WHERE race_id = :race_id\"),\n",
    "                {\"race_id\": race_id}\n",
    "            ).first()\n",
    "\n",
    "            if race_exists:\n",
    "                logger.info(f\"Race {race_id} already loaded ‚Äî skipping\")\n",
    "                continue\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # RACES\n",
    "            # ------------------------------------------------\n",
    "            race_df = pd.DataFrame([{\n",
    "                \"race_id\": race_id,\n",
    "                \"season\": season,\n",
    "                \"round\": round_number,\n",
    "            }])\n",
    "\n",
    "            race_df.to_sql(\n",
    "                \"races\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # RESULTS (authoritative driver source)\n",
    "            # ------------------------------------------------\n",
    "            results_df = pd.read_parquet(race_dir / \"results.parquet\")\n",
    "            results_df[\"race_id\"] = race_id\n",
    "\n",
    "            results_df.to_sql(\n",
    "                \"results\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # DRIVERS (derived from results, race-scoped)\n",
    "            # ------------------------------------------------\n",
    "            drivers_df = (\n",
    "                results_df[[\"race_id\", \"Abbreviation\", \"DriverNumber\"]]\n",
    "                .drop_duplicates()\n",
    "                .rename(columns={\n",
    "                    \"Abbreviation\": \"driver_code\",\n",
    "                    \"DriverNumber\": \"driver_number\",\n",
    "                })\n",
    "            )\n",
    "\n",
    "            drivers_df.to_sql(\n",
    "                \"drivers\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # LAPS (no renaming ‚Äî schema already standardized)\n",
    "            # ------------------------------------------------\n",
    "            laps_df = pd.read_parquet(race_dir / \"laps.parquet\")\n",
    "            laps_df[\"race_id\"] = race_id\n",
    "\n",
    "            laps_df = laps_df[\n",
    "                [\"race_id\", \"Driver\", \"LapNumber\", \"LapTime\", \"Compound\"]\n",
    "            ]\n",
    "\n",
    "            laps_df.to_sql(\n",
    "                \"laps\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # TRACK STATUS (event-based)\n",
    "            # ------------------------------------------------\n",
    "            track_status_df = pd.read_parquet(\n",
    "                race_dir / \"track_status.parquet\"\n",
    "            )\n",
    "            track_status_df[\"race_id\"] = race_id\n",
    "\n",
    "            track_status_df.to_sql(\n",
    "                \"track_status\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # WEATHER DATA\n",
    "            # ------------------------------------------------\n",
    "            weather_df = pd.read_parquet(\n",
    "                race_dir / \"weather_data.parquet\"\n",
    "            )\n",
    "            weather_df[\"race_id\"] = race_id\n",
    "\n",
    "            weather_df.to_sql(\n",
    "                \"weather_data\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # SESSION STATUS\n",
    "            # ------------------------------------------------\n",
    "            session_status_df = pd.read_parquet(\n",
    "                race_dir / \"session_status.parquet\"\n",
    "            )\n",
    "            session_status_df[\"race_id\"] = race_id\n",
    "\n",
    "            session_status_df.to_sql(\n",
    "                \"session_status\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "            # ------------------------------------------------\n",
    "            # RACE CONTROL MESSAGES\n",
    "            # ------------------------------------------------\n",
    "            rcm_df = pd.read_parquet(\n",
    "                race_dir / \"race_control_messages.parquet\"\n",
    "            )\n",
    "            rcm_df[\"race_id\"] = race_id\n",
    "\n",
    "            rcm_df.to_sql(\n",
    "                \"race_control_messages\",\n",
    "                conn,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "\n",
    "logger.info(\"Starting standardized Parquet -> PostgreSQL load (idempotent)\")\n",
    "\n",
    "print(\"\"\"\n",
    "============================================================\n",
    "Notebook 03 ‚Äî Data Load Complete (Idempotent)\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ Existing races safely skipped\n",
    "‚Ä¢ All six standardized tables loaded\n",
    "‚Ä¢ Column names preserved exactly\n",
    "‚Ä¢ Relational invariants respected\n",
    "\n",
    "Notebook 03 is now structurally complete.\n",
    "============================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7919023-e411-4e5f-bcea-730c46a5a5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-31 10:46:06,963 | INFO | src.logging_config | Starting load validation & pipeline readiness checks (Cell 6)\n",
      "2025-12-31 10:46:06,972 | INFO | src.logging_config | Row counts ‚Äî races=68, drivers=1359, laps=74605\n",
      "2025-12-31 10:46:06,983 | INFO | src.logging_config | No orphan laps detected\n",
      "2025-12-31 10:46:07,019 | INFO | src.logging_config | Lap grain verified (race_id, driver_code, lap_number)\n",
      "2025-12-31 10:46:07,026 | INFO | src.logging_config | Standardized track_status.parquet present for all races\n",
      "\n",
      "============================================================\n",
      "Notebook 03 ‚Äî Relational Validation Complete\n",
      "------------------------------------------------------------\n",
      "‚Ä¢ Core relational tables populated\n",
      "‚Ä¢ Referential integrity enforced\n",
      "‚Ä¢ Lap grain is correct and unique\n",
      "‚Ä¢ Track status events available on disk\n",
      "‚Ä¢ Database ready for derived feature computation\n",
      "\n",
      "Next Notebook:\n",
      "  - Compute cumulative lap times\n",
      "  - Derive pit laps and out laps\n",
      "  - Map track status events to lap-level flags\n",
      "  - Enable deterministic undercut detection\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Notebook 03 ‚Äî PostgreSQL Loading & Relational Invariants\n",
    "# Cell 6: Load Validation & Pipeline Readiness\n",
    "# ============================================================\n",
    "\n",
    "from sqlalchemy import text\n",
    "from src.logging_config import setup_logging\n",
    "from src.config import Config\n",
    "\n",
    "logger, _ = setup_logging()\n",
    "logger.info(\"Starting load validation & pipeline readiness checks (Cell 6)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Database-level validation\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "with engine.connect() as conn:\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 1. Basic row counts (sanity, not analytics)\n",
    "    # --------------------------------------------------------\n",
    "    counts = conn.execute(text(\"\"\"\n",
    "        SELECT\n",
    "            (SELECT COUNT(*) FROM races)   AS races,\n",
    "            (SELECT COUNT(*) FROM drivers) AS drivers,\n",
    "            (SELECT COUNT(*) FROM laps)    AS laps\n",
    "    \"\"\")).mappings().one()\n",
    "\n",
    "    logger.info(\n",
    "        f\"Row counts ‚Äî races={counts['races']}, \"\n",
    "        f\"drivers={counts['drivers']}, \"\n",
    "        f\"laps={counts['laps']}\"\n",
    "    )\n",
    "\n",
    "    if counts[\"races\"] == 0:\n",
    "        raise RuntimeError(\"No races loaded ‚Äî pipeline is broken\")\n",
    "\n",
    "    if counts[\"drivers\"] == 0:\n",
    "        raise RuntimeError(\"No drivers loaded ‚Äî identity mapping failed\")\n",
    "\n",
    "    if counts[\"laps\"] == 0:\n",
    "        raise RuntimeError(\"No laps loaded ‚Äî core fact table empty\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 2. Referential integrity: laps ‚Üí drivers\n",
    "    # --------------------------------------------------------\n",
    "    orphan_laps = conn.execute(text(\"\"\"\n",
    "        SELECT COUNT(*) AS orphan_count\n",
    "        FROM laps l\n",
    "        LEFT JOIN drivers d\n",
    "          ON l.race_id = d.race_id\n",
    "         AND l.driver_code = d.driver_code\n",
    "        WHERE d.driver_code IS NULL\n",
    "    \"\"\")).scalar()\n",
    "\n",
    "    if orphan_laps > 0:\n",
    "        raise RuntimeError(\n",
    "            f\"Found {orphan_laps} laps without matching drivers\"\n",
    "        )\n",
    "\n",
    "    logger.info(\"No orphan laps detected\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    # 3. Lap grain validation\n",
    "    # --------------------------------------------------------\n",
    "    duplicate_laps = conn.execute(text(\"\"\"\n",
    "        SELECT COUNT(*) FROM (\n",
    "            SELECT race_id, driver_code, lap_number, COUNT(*) c\n",
    "            FROM laps\n",
    "            GROUP BY race_id, driver_code, lap_number\n",
    "            HAVING COUNT(*) > 1\n",
    "        ) t\n",
    "    \"\"\")).scalar()\n",
    "\n",
    "    if duplicate_laps > 0:\n",
    "        raise RuntimeError(\n",
    "            f\"Duplicate lap rows detected: {duplicate_laps}\"\n",
    "        )\n",
    "\n",
    "    logger.info(\"Lap grain verified (race_id, driver_code, lap_number)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Filesystem-level validation for deferred datasets\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "STANDARDIZED_DIR = Config.DATA_DIR / \"interim\" / \"standardized\"\n",
    "\n",
    "missing_track_status = []\n",
    "\n",
    "for year_dir in STANDARDIZED_DIR.glob(\"year=*\"):\n",
    "    for race_dir in year_dir.iterdir():\n",
    "        if not (race_dir / \"track_status.parquet\").exists():\n",
    "            missing_track_status.append(str(race_dir))\n",
    "\n",
    "if missing_track_status:\n",
    "    raise RuntimeError(\n",
    "        \"Standardized track_status.parquet missing for races:\\n\"\n",
    "        + \"\\n\".join(missing_track_status[:5])\n",
    "        + (\"\\n...\" if len(missing_track_status) > 5 else \"\")\n",
    "    )\n",
    "\n",
    "logger.info(\"Standardized track_status.parquet present for all races\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Final readiness confirmation\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\"\"\n",
    "============================================================\n",
    "Notebook 03 ‚Äî Relational Validation Complete\n",
    "------------------------------------------------------------\n",
    "‚Ä¢ Core relational tables populated\n",
    "‚Ä¢ Referential integrity enforced\n",
    "‚Ä¢ Lap grain is correct and unique\n",
    "‚Ä¢ Track status events available on disk\n",
    "‚Ä¢ Database ready for derived feature computation\n",
    "\n",
    "Next Notebook:\n",
    "  - Compute cumulative lap times\n",
    "  - Derive pit laps and out laps\n",
    "  - Map track status events to lap-level flags\n",
    "  - Enable deterministic undercut detection\n",
    "============================================================\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bda933-cb35-4018-a763-7b5115e6f8d1",
   "metadata": {},
   "source": [
    "# üß† Notebook 03 ‚Äî Conclusion, Error Log & Forward Plan\n",
    "\n",
    "## üßæ High-Level Summary\n",
    "\n",
    "Notebook 03 was **not a smooth execution notebook** ‚Äî and that is precisely why it is one of the most important ones.\n",
    "\n",
    "What initially appeared to be a simple *‚ÄúParquet ‚Üí PostgreSQL load‚Äù* uncovered multiple hidden assumptions across schema, grain, identity, and execution semantics.\n",
    "\n",
    "Rather than bypassing these issues, each was surfaced, diagnosed, and resolved deliberately ‚Äî strengthening the pipeline instead of weakening it.\n",
    "\n",
    "Notebook 03 is where the project transitioned from *clean files* to a **verified relational system**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç What We Built (Final State)\n",
    "\n",
    "At completion, we achieved:\n",
    "\n",
    "üìä **Relational Tables (Materialized in PostgreSQL)**\n",
    "- `races`\n",
    "- `drivers`\n",
    "- `laps`\n",
    "\n",
    "üìÇ **Standardized & Validated Datasets (On Disk)**\n",
    "- `track_status`\n",
    "- `weather_data`\n",
    "- `session_status`\n",
    "- `race_control_messages`\n",
    "\n",
    "üîí **Verified Invariants (via logic + validation)**\n",
    "- No duplicate races\n",
    "- No duplicate drivers per race\n",
    "- No duplicate laps\n",
    "- No orphan lap records\n",
    "- Stable race identity across re-runs\n",
    "\n",
    "üîÅ **Idempotent Execution**\n",
    "- Kernel restarts are safe\n",
    "- Full notebook re-runs are safe\n",
    "- Existing races are detected and skipped deterministically\n",
    "\n",
    "PostgreSQL now acts as a **structural backbone**, not a transformation layer.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Errors & Problems Encountered (Chronological)\n",
    "\n",
    "### ‚ùå Column Name Assumptions\n",
    "- Expected `driver_code`, found `Driver` / `Abbreviation`\n",
    "- Resolved by inspecting actual standardized Parquet schemas\n",
    "- Reinforced the rule: *never assume column names downstream*\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå NOT NULL Violations\n",
    "- Missing `round` field in `races`\n",
    "- Fixed by deriving race identity explicitly from directory structure\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Duplicate Primary Keys\n",
    "- Occurred during notebook re-runs\n",
    "- Resolved through explicit idempotency checks and conflict-safe inserts\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Track Status Grain Errors\n",
    "- Early attempts treated track status as lap-level data\n",
    "- Led to null lap numbers and invalid joins\n",
    "- Realization: **track status is event-based, not lap-based**\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Logger Misuse\n",
    "- `setup_logging()` returns `(logger, error_logger)`\n",
    "- Incorrect unpacking caused runtime failures\n",
    "- Fixed via explicit tuple handling and consistent logger scope\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Scope Errors in Logging\n",
    "- Logging attempted after conditional skips\n",
    "- Removed unsafe debug lines to preserve execution safety\n",
    "\n",
    "Each failure exposed an assumption.  \n",
    "Each fix tightened the system.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Validation Results (Cell 6)\n",
    "\n",
    "Final validation confirmed:\n",
    "\n",
    "üìà **Row Counts**\n",
    "- 68 races\n",
    "- 1,359 drivers\n",
    "- 74,605 laps\n",
    "\n",
    "üß¨ **Relational Integrity**\n",
    "- No orphan laps\n",
    "- Correct lap grain: `(race_id, driver_code, lap_number)`\n",
    "- No duplicate fact rows\n",
    "\n",
    "üìÇ **Deferred Datasets**\n",
    "- `track_status.parquet` present for every race on disk\n",
    "- Guaranteed availability for downstream temporal mapping\n",
    "\n",
    "The system is now **analytically trustworthy**, not just executable.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† How Our Understanding Changed\n",
    "\n",
    "### üîÅ Track Status Reframed\n",
    "Initially treated as a fact table ‚Äî now correctly understood as:\n",
    "\n",
    "> A temporal event stream that must be mapped *after* lap timelines exist.\n",
    "\n",
    "This reframing eliminated grain violations and unlocked clean temporal reasoning.\n",
    "\n",
    "---\n",
    "\n",
    "### üß± PostgreSQL Repositioned\n",
    "PostgreSQL is **not** a data-cleaning engine in this project.\n",
    "\n",
    "It is:\n",
    "- A structural enforcer\n",
    "- A validation boundary\n",
    "- A restart-safe analytical backbone\n",
    "\n",
    "All semantic transformations remain upstream or downstream by design.\n",
    "\n",
    "---\n",
    "\n",
    "### üß≠ Scope Tightened\n",
    "Notebook 03 ends **before any strategy logic** intentionally.\n",
    "\n",
    "No:\n",
    "- Pit detection\n",
    "- Cumulative time computation\n",
    "- Undercut logic\n",
    "\n",
    "Those require time-aware context and belong downstream.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ NEXT STEPS ‚Äî Notebook 04 (CRITICAL)\n",
    "\n",
    "Notebook 04 is where **time finally enters the system**.\n",
    "\n",
    "### üìå Notebook 04 ‚Äî Cumulative Lap Timelines & Temporal Mapping\n",
    "\n",
    "#### 1Ô∏è‚É£ Compute cumulative race time per driver\n",
    "Using:\n",
    "- `laps.lap_time_ms`\n",
    "- Ordered by `(race_id, driver_code, lap_number)`\n",
    "\n",
    "Produces:\n",
    "- `lap_start_time_ms`\n",
    "- `lap_end_time_ms`\n",
    "\n",
    "This transforms laps from discrete rows into **continuous timelines**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2Ô∏è‚É£ Identify pit laps & out laps\n",
    "Using:\n",
    "- Lap time spikes\n",
    "- Tyre compound changes\n",
    "- Stint boundary detection\n",
    "\n",
    "Produces:\n",
    "- `is_pit_lap`\n",
    "- `is_out_lap`\n",
    "- `stint_id`\n",
    "\n",
    "---\n",
    "\n",
    "#### 3Ô∏è‚É£ Load & map track status events üö¶\n",
    "- Read `track_status.parquet`\n",
    "- Convert events into time ranges\n",
    "- Overlay onto lap timelines\n",
    "\n",
    "Produces:\n",
    "- `is_green_lap`\n",
    "- `is_sc_lap`\n",
    "- `is_vsc_lap`\n",
    "\n",
    "‚ö†Ô∏è This step is **impossible before Notebook 04**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4Ô∏è‚É£ Filter analytically valid laps\n",
    "Exclude:\n",
    "- Safety Car laps\n",
    "- Red flag laps\n",
    "- Out laps\n",
    "\n",
    "Preserve:\n",
    "- Competitive green-flag laps only\n",
    "\n",
    "---\n",
    "\n",
    "#### 5Ô∏è‚É£ Prepare deterministic undercut inputs\n",
    "At the end of Notebook 04, we will have:\n",
    "\n",
    "- Clean lap deltas\n",
    "- Valid stint transitions\n",
    "- Strategy-ready lap windows\n",
    "\n",
    "‚ö†Ô∏è **No undercut detection yet**  \n",
    "That belongs strictly to Notebook 05.\n",
    "\n",
    "---\n",
    "\n",
    "## üèÅ Final Reflection\n",
    "\n",
    "Notebook 03 transformed the project from:\n",
    "\n",
    "> ‚ÄúClean files on disk‚Äù\n",
    "\n",
    "into:\n",
    "\n",
    "> ‚ÄúA verified relational system that can survive scrutiny, restarts, and structural enforcement.‚Äù\n",
    "\n",
    "Every error exposed a hidden assumption.  \n",
    "Every fix strengthened the architecture.\n",
    "\n",
    "With this foundation complete, **strategy analysis can now begin ‚Äî safely, reproducibly, and without shortcuts**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
