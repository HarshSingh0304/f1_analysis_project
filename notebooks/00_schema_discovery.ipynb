{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f63289e5-92aa-477b-ae1a-45d3930ee768",
   "metadata": {},
   "source": [
    "# üìò Notebook 00 ‚Äî Schema Discovery & API Ground-Truthing\n",
    "\n",
    "## üéØ Purpose of This Notebook\n",
    "\n",
    "This notebook is the **foundational starting point** of the entire Formula 1 end-to-end data analytics project.  \n",
    "Before writing *any* ingestion, transformation, database, or analytics code, we must answer one critical question:\n",
    "\n",
    "> **What data does the FastF1 API actually provide ‚Äî structurally, consistently, and reliably?**\n",
    "\n",
    "This notebook exists to answer that question **empirically**, using real API responses ‚Äî not assumptions, guesses, or partial documentation.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Why Schema Discovery Is Necessary\n",
    "\n",
    "APIs (especially analytics-focused APIs like FastF1):\n",
    "\n",
    "- Change over time\n",
    "- Contain lazy-loaded objects\n",
    "- Expose partially documented attributes\n",
    "- Behave differently across seasons\n",
    "\n",
    "Building pipelines **without validating the real schema** leads to:\n",
    "\n",
    "- Runtime failures in later notebooks\n",
    "- Silent data corruption\n",
    "- Fragile analytics code\n",
    "- Database schema mismatches\n",
    "\n",
    "This notebook prevents those issues by **locking down schema truth first**.\n",
    "\n",
    "---\n",
    "\n",
    "## üìå Scope of This Notebook\n",
    "\n",
    "This notebook focuses **only on schema discovery**.\n",
    "\n",
    "üö´ It explicitly does **not** perform:\n",
    "- Data cleaning\n",
    "- Data normalization\n",
    "- Feature engineering\n",
    "- Business logic\n",
    "- Database insertion\n",
    "- Analytical modeling\n",
    "\n",
    "Those steps are intentionally deferred to later notebooks.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è What We Will Do in This Notebook\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "### 1Ô∏è‚É£ Initialize the Project Environment\n",
    "- Resolve project root paths robustly (Jupyter-safe)\n",
    "- Configure structured logging\n",
    "- Initialize the FastF1 API with cache-aware defaults\n",
    "\n",
    "### 2Ô∏è‚É£ Discover Schema-Bearing Objects\n",
    "- Load a **minimal but representative dataset**  \n",
    "  *(Bahrain Grand Prix ‚Äî Race sessions for 2022, 2023, 2024)*\n",
    "- Identify which FastF1 session attributes behave like tables\n",
    "- Validate that these objects exist consistently across years\n",
    "\n",
    "### 3Ô∏è‚É£ Materialize and Inspect Real Data Structures\n",
    "- Convert discovered objects into Pandas DataFrames\n",
    "- Extract:\n",
    "  - Column names\n",
    "  - Data types\n",
    "  - Row counts\n",
    "- Verify schema stability across seasons\n",
    "\n",
    "### 4Ô∏è‚É£ Persist Schema Metadata\n",
    "- Store schema information as JSON artifacts\n",
    "- Use these artifacts as **contracts** for downstream notebooks\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Expected Outputs\n",
    "\n",
    "By the end of this notebook, we expect to produce:\n",
    "\n",
    "- üìÑ **Schema discovery metadata**\n",
    "  - Which tables exist\n",
    "  - Which are stable across seasons\n",
    "\n",
    "- üìÑ **Column-level schema contracts**\n",
    "  - Exact column names\n",
    "  - Pandas data types\n",
    "  - Row counts per year\n",
    "\n",
    "These outputs ensure that **all future notebooks can be written without guessing**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© How This Notebook Fits into the Full Pipeline\n",
    "\n",
    "This notebook establishes the **ground truth** for the entire project:\n",
    "\n",
    "| Stage | Responsibility |\n",
    "|-----|---------------|\n",
    "| **Notebook 00** | Schema discovery & validation |\n",
    "| Notebook 01 | Multi-year raw data ingestion |\n",
    "| Notebook 02 | Data type standardization & normalization |\n",
    "| Notebook 03 | Data modeling & relational structure |\n",
    "| Notebook 04 | PostgreSQL loading |\n",
    "| Notebook 05+ | Analytics & insights |\n",
    "\n",
    "Nothing downstream should contradict what is discovered here.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Success Criteria\n",
    "\n",
    "This notebook is considered complete when:\n",
    "- All tables are known\n",
    "- All column names are known\n",
    "- All data types are known\n",
    "- Schemas are verified across multiple seasons\n",
    "- No assumptions remain for future notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f096f5-4561-421d-a795-d978b011005d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Project root added to sys.path: C:\\Users\\hersh\\Desktop\\f1_analysis_project\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 13:53:54,732 | INFO | src.logging_config | Notebook 00 ‚Äî Schema discovery started\n",
      "2025-12-14 13:53:54,734 | INFO | src.logging_config | Project root resolved at: C:\\Users\\hersh\\Desktop\\f1_analysis_project\n",
      "2025-12-14 13:53:54,737 | INFO | src.logging_config | Configuration loaded successfully\n",
      "2025-12-14 13:53:54,739 | INFO | src.logging_config | Database host: localhost\n",
      "2025-12-14 13:53:54,740 | INFO | src.logging_config | Database name: f1_analysis\n",
      "2025-12-14 13:53:54,748 | INFO | src.logging_config | Interim data directory ready: C:\\Users\\hersh\\Desktop\\f1_analysis_project\\data\\interim\n",
      "2025-12-14 13:53:54,751 | INFO | src.logging_config | Raw data directory ready: C:\\Users\\hersh\\Desktop\\f1_analysis_project\\data\\raw\n",
      "2025-12-14 13:53:54,774 | INFO | src.logging_config | FastF1 initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment bootstrap completed successfully\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 1 ‚Äî Environment bootstrap, logging, and FastF1 setup\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Resolve and register project root\n",
    "# ------------------------------------------------------------\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[0]\n",
    "\n",
    "if not PROJECT_ROOT.exists():\n",
    "    raise RuntimeError(\"Project root could not be resolved.\")\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"‚úÖ Project root added to sys.path: {PROJECT_ROOT}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Import internal modules\n",
    "# ------------------------------------------------------------\n",
    "from src.config import Config\n",
    "from src.logging_config import setup_logging\n",
    "from src.fastf1_client import setup_fastf1\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Initialize logging\n",
    "# ------------------------------------------------------------\n",
    "logger, error_logger = setup_logging()\n",
    "\n",
    "logger.info(\"Notebook 00 ‚Äî Schema discovery started\")\n",
    "logger.info(f\"Project root resolved at: {PROJECT_ROOT}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Load configuration (.env)\n",
    "# ------------------------------------------------------------\n",
    "config = Config()\n",
    "\n",
    "logger.info(\"Configuration loaded successfully\")\n",
    "logger.info(f\"Database host: {config.DB_HOST}\")\n",
    "logger.info(f\"Database name: {config.DB_NAME}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Ensure data directories exist\n",
    "# ------------------------------------------------------------\n",
    "INTERIM_DIR = PROJECT_ROOT / \"data\" / \"interim\"\n",
    "RAW_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info(f\"Interim data directory ready: {INTERIM_DIR}\")\n",
    "logger.info(f\"Raw data directory ready: {RAW_DIR}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Initialize FastF1 (cache handled internally)\n",
    "# ------------------------------------------------------------\n",
    "setup_fastf1()\n",
    "\n",
    "logger.info(\"FastF1 initialized successfully\")\n",
    "\n",
    "print(\"‚úÖ Environment bootstrap completed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a70cb40b-9c3d-4524-9ce0-c4b8ebbb4e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 13:53:54,800 | INFO | src.logging_config | Cell 2 ‚Äî Pure schema discovery started (root-anchored, zero assumptions)\n",
      "2025-12-14 13:53:54,804 | INFO | src.logging_config | Project root resolved at: C:\\Users\\hersh\\Desktop\\f1_analysis_project\n",
      "2025-12-14 13:53:54,806 | INFO | src.logging_config | Schema output path: C:\\Users\\hersh\\Desktop\\f1_analysis_project\\data\\interim\\schema_discovery_fastf1_bahrain_2022_2024.json\n",
      "2025-12-14 13:53:54,808 | INFO | src.batch_loader | Loading Bahrain 2022\n",
      "core           INFO \tLoading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "2025-12-14 13:53:57,033 | INFO | fastf1.fastf1.core | Loading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "2025-12-14 13:53:57,038 | INFO | fastf1.fastf1.req | Using cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "2025-12-14 13:53:57,042 | INFO | fastf1.fastf1.req | Using cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "2025-12-14 13:53:57,125 | INFO | fastf1.fastf1.req | Using cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "2025-12-14 13:53:57,132 | INFO | fastf1.fastf1.req | Using cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "2025-12-14 13:53:57,137 | INFO | fastf1.fastf1.req | Using cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "2025-12-14 13:53:57,150 | INFO | fastf1.fastf1.req | Using cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "2025-12-14 13:53:57,156 | INFO | fastf1.fastf1.req | Using cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "2025-12-14 13:53:57,159 | INFO | fastf1.fastf1.core | Processing timing data...\n",
      "req            INFO \tUsing cached data for car_data\n",
      "2025-12-14 13:54:00,498 | INFO | fastf1.fastf1.req | Using cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "2025-12-14 13:54:00,823 | INFO | fastf1.fastf1.req | Using cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "2025-12-14 13:54:05,834 | INFO | fastf1.fastf1.req | Using cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "2025-12-14 13:54:05,841 | INFO | fastf1.fastf1.req | Using cached data for race_control_messages\n",
      "core        WARNING \tDriver 16 completed the race distance 00:00.050000 before the recorded end of the session.\n",
      "2025-12-14 13:54:05,860 | WARNING | fastf1.fastf1.core | Driver 16 completed the race distance 00:00.050000 before the recorded end of the session.\n",
      "core           INFO \tFinished loading data for 20 drivers: ['16', '55', '44', '63', '20', '77', '31', '22', '14', '24', '47', '18', '23', '3', '4', '6', '27', '11', '1', '10']\n",
      "2025-12-14 13:54:06,182 | INFO | fastf1.fastf1.core | Finished loading data for 20 drivers: ['16', '55', '44', '63', '20', '77', '31', '22', '14', '24', '47', '18', '23', '3', '4', '6', '27', '11', '1', '10']\n",
      "2025-12-14 13:54:06,185 | INFO | src.batch_loader | Loading Bahrain 2023\n",
      "core           INFO \tLoading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "2025-12-14 13:54:07,656 | INFO | fastf1.fastf1.core | Loading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "2025-12-14 13:54:07,662 | INFO | fastf1.fastf1.req | Using cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "2025-12-14 13:54:07,667 | INFO | fastf1.fastf1.req | Using cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "2025-12-14 13:54:07,742 | INFO | fastf1.fastf1.req | Using cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "2025-12-14 13:54:07,751 | INFO | fastf1.fastf1.req | Using cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "2025-12-14 13:54:07,755 | INFO | fastf1.fastf1.req | Using cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "2025-12-14 13:54:07,776 | INFO | fastf1.fastf1.req | Using cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "2025-12-14 13:54:07,782 | INFO | fastf1.fastf1.req | Using cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "2025-12-14 13:54:07,785 | INFO | fastf1.fastf1.core | Processing timing data...\n",
      "req            INFO \tUsing cached data for car_data\n",
      "2025-12-14 13:54:10,905 | INFO | fastf1.fastf1.req | Using cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "2025-12-14 13:54:11,169 | INFO | fastf1.fastf1.req | Using cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "2025-12-14 13:54:15,617 | INFO | fastf1.fastf1.req | Using cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "2025-12-14 13:54:15,626 | INFO | fastf1.fastf1.req | Using cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '11', '14', '55', '44', '18', '63', '77', '10', '23', '22', '2', '20', '21', '27', '24', '4', '31', '16', '81']\n",
      "2025-12-14 13:54:15,930 | INFO | fastf1.fastf1.core | Finished loading data for 20 drivers: ['1', '11', '14', '55', '44', '18', '63', '77', '10', '23', '22', '2', '20', '21', '27', '24', '4', '31', '16', '81']\n",
      "2025-12-14 13:54:15,934 | INFO | src.batch_loader | Loading Bahrain 2024\n",
      "core           INFO \tLoading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "2025-12-14 13:54:17,803 | INFO | fastf1.fastf1.core | Loading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "2025-12-14 13:54:17,808 | INFO | fastf1.fastf1.req | Using cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "2025-12-14 13:54:17,813 | INFO | fastf1.fastf1.req | Using cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "2025-12-14 13:54:17,890 | INFO | fastf1.fastf1.req | Using cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "2025-12-14 13:54:17,895 | INFO | fastf1.fastf1.req | Using cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "2025-12-14 13:54:17,898 | INFO | fastf1.fastf1.req | Using cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "2025-12-14 13:54:17,919 | INFO | fastf1.fastf1.req | Using cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "2025-12-14 13:54:17,927 | INFO | fastf1.fastf1.req | Using cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "2025-12-14 13:54:17,930 | INFO | fastf1.fastf1.core | Processing timing data...\n",
      "req            INFO \tUsing cached data for car_data\n",
      "2025-12-14 13:54:21,083 | INFO | fastf1.fastf1.req | Using cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "2025-12-14 13:54:21,326 | INFO | fastf1.fastf1.req | Using cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "2025-12-14 13:54:25,582 | INFO | fastf1.fastf1.req | Using cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "2025-12-14 13:54:25,589 | INFO | fastf1.fastf1.req | Using cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '11', '55', '16', '63', '4', '44', '81', '14', '18', '24', '20', '3', '22', '23', '27', '31', '10', '77', '2']\n",
      "2025-12-14 13:54:25,841 | INFO | fastf1.fastf1.core | Finished loading data for 20 drivers: ['1', '11', '55', '16', '63', '4', '44', '81', '14', '18', '24', '20', '3', '22', '23', '27', '31', '10', '77', '2']\n",
      "2025-12-14 13:54:25,845 | INFO | src.logging_config | 3 sessions loaded for schema inspection\n",
      "2025-12-14 13:54:25,849 | INFO | src.logging_config | Introspecting session object for year 2022\n",
      "2025-12-14 13:54:25,867 | INFO | src.logging_config | Year 2022 ‚Äî discovered 8 schema-bearing attributes\n",
      "2025-12-14 13:54:25,868 | INFO | src.logging_config | Introspecting session object for year 2023\n",
      "2025-12-14 13:54:25,889 | INFO | src.logging_config | Year 2023 ‚Äî discovered 8 schema-bearing attributes\n",
      "2025-12-14 13:54:25,891 | INFO | src.logging_config | Introspecting session object for year 2024\n",
      "2025-12-14 13:54:25,911 | INFO | src.logging_config | Year 2024 ‚Äî discovered 8 schema-bearing attributes\n",
      "2025-12-14 13:54:25,931 | INFO | src.logging_config | Schema discovery metadata written successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cell 2 completed successfully ‚Äî schema metadata persisted\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 2 ‚Äî Pure schema discovery (root-anchored, zero assumptions)\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from src.batch_loader import load_multiple_years\n",
    "\n",
    "logger.info(\"Cell 2 ‚Äî Pure schema discovery started (root-anchored, zero assumptions)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 0. Resolve PROJECT ROOT explicitly (CRITICAL FIX)\n",
    "# ------------------------------------------------------------\n",
    "# Notebook runs from: project_root/notebooks/\n",
    "# So project root is one level up\n",
    "PROJECT_ROOT = Path.cwd().resolve().parents[0]\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "INTERIM_DIR = DATA_DIR / \"interim\"\n",
    "INTERIM_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SCHEMA_OUTPUT_PATH = (\n",
    "    INTERIM_DIR / \"schema_discovery_fastf1_bahrain_2022_2024.json\"\n",
    ")\n",
    "\n",
    "logger.info(f\"Project root resolved at: {PROJECT_ROOT}\")\n",
    "logger.info(f\"Schema output path: {SCHEMA_OUTPUT_PATH}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Load sessions (FastF1 cache-aware, no schema assumptions)\n",
    "# ------------------------------------------------------------\n",
    "YEARS = [2022, 2023, 2024]\n",
    "GP_NAME = \"Bahrain\"\n",
    "\n",
    "try:\n",
    "    sessions = load_multiple_years(YEARS, GP_NAME)\n",
    "except Exception:\n",
    "    error_logger.error(\n",
    "        \"Failed to load sessions for schema discovery\", exc_info=True\n",
    "    )\n",
    "    raise\n",
    "\n",
    "logger.info(f\"{len(sessions)} sessions loaded for schema inspection\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Helper: extract DataFrame schema safely\n",
    "# ------------------------------------------------------------\n",
    "def extract_dataframe_schema(df: pd.DataFrame) -> dict:\n",
    "    return {\n",
    "        \"columns\": list(df.columns),\n",
    "        \"dtypes\": {col: str(dtype) for col, dtype in df.dtypes.items()},\n",
    "        \"row_count\": int(len(df)),\n",
    "    }\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Introspect FastF1 Session objects dynamically\n",
    "# ------------------------------------------------------------\n",
    "schema_discovery = {}\n",
    "\n",
    "for session in sessions:\n",
    "    year = session.event.year\n",
    "    logger.info(f\"Introspecting session object for year {year}\")\n",
    "\n",
    "    schema_discovery[year] = {}\n",
    "\n",
    "    for attr_name in dir(session):\n",
    "        if attr_name.startswith(\"_\"):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            attr_value = getattr(session, attr_name)\n",
    "        except Exception:\n",
    "            # Some attributes intentionally fail on access\n",
    "            continue\n",
    "\n",
    "        # Case 1: Attribute is a DataFrame\n",
    "        if isinstance(attr_value, pd.DataFrame):\n",
    "            schema_discovery[year][attr_name] = {\n",
    "                \"object_type\": \"DataFrame\",\n",
    "                \"schema\": extract_dataframe_schema(attr_value),\n",
    "            }\n",
    "\n",
    "        # Case 2: Attribute is a dict of DataFrames\n",
    "        elif isinstance(attr_value, dict):\n",
    "            df_entries = {\n",
    "                key: extract_dataframe_schema(val)\n",
    "                for key, val in attr_value.items()\n",
    "                if isinstance(val, pd.DataFrame)\n",
    "            }\n",
    "\n",
    "            if df_entries:\n",
    "                schema_discovery[year][attr_name] = {\n",
    "                    \"object_type\": \"Dict[str, DataFrame]\",\n",
    "                    \"entries\": df_entries,\n",
    "                }\n",
    "\n",
    "    logger.info(\n",
    "        f\"Year {year} ‚Äî discovered \"\n",
    "        f\"{len(schema_discovery[year])} schema-bearing attributes\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Persist schema metadata (INTERIM only)\n",
    "# ------------------------------------------------------------\n",
    "with open(SCHEMA_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema_discovery, f, indent=2)\n",
    "\n",
    "logger.info(\"Schema discovery metadata written successfully\")\n",
    "print(\"‚úÖ Cell 2 completed successfully ‚Äî schema metadata persisted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "651c4ac5-c0e0-4f11-8801-bee3c93735d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 14:11:36,529 | INFO | src.logging_config | Cell 3 ‚Äî Column-level schema materialization started\n",
      "2025-12-14 14:11:36,531 | INFO | src.logging_config | Project root resolved at: C:\\Users\\hersh\\Desktop\\f1_analysis_project\n",
      "2025-12-14 14:11:36,533 | INFO | src.logging_config | Loading schema discovery from: C:\\Users\\hersh\\Desktop\\f1_analysis_project\\data\\interim\\schema_discovery_fastf1_bahrain_2022_2024.json\n",
      "2025-12-14 14:11:36,539 | INFO | src.logging_config | Schema discovery metadata loaded successfully\n",
      "2025-12-14 14:11:36,542 | INFO | src.batch_loader | Loading Bahrain 2022\n",
      "core           INFO \tLoading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "2025-12-14 14:11:38,795 | INFO | fastf1.fastf1.core | Loading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "2025-12-14 14:11:38,800 | INFO | fastf1.fastf1.req | Using cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "2025-12-14 14:11:38,804 | INFO | fastf1.fastf1.req | Using cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "2025-12-14 14:11:38,868 | INFO | fastf1.fastf1.req | Using cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "2025-12-14 14:11:38,872 | INFO | fastf1.fastf1.req | Using cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "2025-12-14 14:11:38,875 | INFO | fastf1.fastf1.req | Using cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "2025-12-14 14:11:38,893 | INFO | fastf1.fastf1.req | Using cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "2025-12-14 14:11:38,899 | INFO | fastf1.fastf1.req | Using cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "2025-12-14 14:11:38,903 | INFO | fastf1.fastf1.core | Processing timing data...\n",
      "req            INFO \tUsing cached data for car_data\n",
      "2025-12-14 14:11:42,080 | INFO | fastf1.fastf1.req | Using cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "2025-12-14 14:11:42,356 | INFO | fastf1.fastf1.req | Using cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "2025-12-14 14:11:47,065 | INFO | fastf1.fastf1.req | Using cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "2025-12-14 14:11:47,076 | INFO | fastf1.fastf1.req | Using cached data for race_control_messages\n",
      "core        WARNING \tDriver 16 completed the race distance 00:00.050000 before the recorded end of the session.\n",
      "2025-12-14 14:11:47,091 | WARNING | fastf1.fastf1.core | Driver 16 completed the race distance 00:00.050000 before the recorded end of the session.\n",
      "core           INFO \tFinished loading data for 20 drivers: ['16', '55', '44', '63', '20', '77', '31', '22', '14', '24', '47', '18', '23', '3', '4', '6', '27', '11', '1', '10']\n",
      "2025-12-14 14:11:47,412 | INFO | fastf1.fastf1.core | Finished loading data for 20 drivers: ['16', '55', '44', '63', '20', '77', '31', '22', '14', '24', '47', '18', '23', '3', '4', '6', '27', '11', '1', '10']\n",
      "2025-12-14 14:11:47,416 | INFO | src.batch_loader | Loading Bahrain 2023\n",
      "core           INFO \tLoading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "2025-12-14 14:11:49,140 | INFO | fastf1.fastf1.core | Loading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "2025-12-14 14:11:49,144 | INFO | fastf1.fastf1.req | Using cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "2025-12-14 14:11:49,147 | INFO | fastf1.fastf1.req | Using cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "2025-12-14 14:11:49,220 | INFO | fastf1.fastf1.req | Using cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "2025-12-14 14:11:49,226 | INFO | fastf1.fastf1.req | Using cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "2025-12-14 14:11:49,231 | INFO | fastf1.fastf1.req | Using cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "2025-12-14 14:11:49,252 | INFO | fastf1.fastf1.req | Using cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "2025-12-14 14:11:49,259 | INFO | fastf1.fastf1.req | Using cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "2025-12-14 14:11:49,263 | INFO | fastf1.fastf1.core | Processing timing data...\n",
      "req            INFO \tUsing cached data for car_data\n",
      "2025-12-14 14:11:52,354 | INFO | fastf1.fastf1.req | Using cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "2025-12-14 14:11:52,632 | INFO | fastf1.fastf1.req | Using cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "2025-12-14 14:11:57,131 | INFO | fastf1.fastf1.req | Using cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "2025-12-14 14:11:57,139 | INFO | fastf1.fastf1.req | Using cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '11', '14', '55', '44', '18', '63', '77', '10', '23', '22', '2', '20', '21', '27', '24', '4', '31', '16', '81']\n",
      "2025-12-14 14:11:57,434 | INFO | fastf1.fastf1.core | Finished loading data for 20 drivers: ['1', '11', '14', '55', '44', '18', '63', '77', '10', '23', '22', '2', '20', '21', '27', '24', '4', '31', '16', '81']\n",
      "2025-12-14 14:11:57,438 | INFO | src.batch_loader | Loading Bahrain 2024\n",
      "core           INFO \tLoading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "2025-12-14 14:11:59,177 | INFO | fastf1.fastf1.core | Loading data for Bahrain Grand Prix - Race [v3.7.0]\n",
      "req            INFO \tUsing cached data for session_info\n",
      "2025-12-14 14:11:59,182 | INFO | fastf1.fastf1.req | Using cached data for session_info\n",
      "req            INFO \tUsing cached data for driver_info\n",
      "2025-12-14 14:11:59,184 | INFO | fastf1.fastf1.req | Using cached data for driver_info\n",
      "req            INFO \tUsing cached data for session_status_data\n",
      "2025-12-14 14:11:59,250 | INFO | fastf1.fastf1.req | Using cached data for session_status_data\n",
      "req            INFO \tUsing cached data for lap_count\n",
      "2025-12-14 14:11:59,256 | INFO | fastf1.fastf1.req | Using cached data for lap_count\n",
      "req            INFO \tUsing cached data for track_status_data\n",
      "2025-12-14 14:11:59,260 | INFO | fastf1.fastf1.req | Using cached data for track_status_data\n",
      "req            INFO \tUsing cached data for _extended_timing_data\n",
      "2025-12-14 14:11:59,283 | INFO | fastf1.fastf1.req | Using cached data for _extended_timing_data\n",
      "req            INFO \tUsing cached data for timing_app_data\n",
      "2025-12-14 14:11:59,288 | INFO | fastf1.fastf1.req | Using cached data for timing_app_data\n",
      "core           INFO \tProcessing timing data...\n",
      "2025-12-14 14:11:59,291 | INFO | fastf1.fastf1.core | Processing timing data...\n",
      "req            INFO \tUsing cached data for car_data\n",
      "2025-12-14 14:12:02,465 | INFO | fastf1.fastf1.req | Using cached data for car_data\n",
      "req            INFO \tUsing cached data for position_data\n",
      "2025-12-14 14:12:02,719 | INFO | fastf1.fastf1.req | Using cached data for position_data\n",
      "req            INFO \tUsing cached data for weather_data\n",
      "2025-12-14 14:12:07,775 | INFO | fastf1.fastf1.req | Using cached data for weather_data\n",
      "req            INFO \tUsing cached data for race_control_messages\n",
      "2025-12-14 14:12:07,785 | INFO | fastf1.fastf1.req | Using cached data for race_control_messages\n",
      "core           INFO \tFinished loading data for 20 drivers: ['1', '11', '55', '16', '63', '4', '44', '81', '14', '18', '24', '20', '3', '22', '23', '27', '31', '10', '77', '2']\n",
      "2025-12-14 14:12:08,115 | INFO | fastf1.fastf1.core | Finished loading data for 20 drivers: ['1', '11', '55', '16', '63', '4', '44', '81', '14', '18', '24', '20', '3', '22', '23', '27', '31', '10', '77', '2']\n",
      "2025-12-14 14:12:08,119 | INFO | src.logging_config | 3 sessions loaded for schema materialization\n",
      "2025-12-14 14:12:08,122 | INFO | src.logging_config | Materializing tables for year 2022\n",
      "2025-12-14 14:12:08,126 | INFO | src.logging_config | laps | 2022 | rows=1125 | cols=31\n",
      "2025-12-14 14:12:08,128 | INFO | src.logging_config | race_control_messages | 2022 | rows=60 | cols=9\n",
      "2025-12-14 14:12:08,132 | INFO | src.logging_config | results | 2022 | rows=20 | cols=22\n",
      "2025-12-14 14:12:08,134 | INFO | src.logging_config | session_status | 2022 | rows=5 | cols=2\n",
      "2025-12-14 14:12:08,136 | INFO | src.logging_config | track_status | 2022 | rows=7 | cols=3\n",
      "2025-12-14 14:12:08,138 | INFO | src.logging_config | weather_data | 2022 | rows=163 | cols=8\n",
      "2025-12-14 14:12:08,142 | INFO | src.logging_config | Materializing tables for year 2023\n",
      "2025-12-14 14:12:08,147 | INFO | src.logging_config | laps | 2023 | rows=1056 | cols=31\n",
      "2025-12-14 14:12:08,150 | INFO | src.logging_config | race_control_messages | 2023 | rows=73 | cols=9\n",
      "2025-12-14 14:12:08,152 | INFO | src.logging_config | results | 2023 | rows=20 | cols=22\n",
      "2025-12-14 14:12:08,154 | INFO | src.logging_config | session_status | 2023 | rows=5 | cols=2\n",
      "2025-12-14 14:12:08,157 | INFO | src.logging_config | track_status | 2023 | rows=11 | cols=3\n",
      "2025-12-14 14:12:08,160 | INFO | src.logging_config | weather_data | 2023 | rows=161 | cols=8\n",
      "2025-12-14 14:12:08,162 | INFO | src.logging_config | Materializing tables for year 2024\n",
      "2025-12-14 14:12:08,169 | INFO | src.logging_config | laps | 2024 | rows=1129 | cols=31\n",
      "2025-12-14 14:12:08,172 | INFO | src.logging_config | race_control_messages | 2024 | rows=69 | cols=9\n",
      "2025-12-14 14:12:08,175 | INFO | src.logging_config | results | 2024 | rows=20 | cols=22\n",
      "2025-12-14 14:12:08,177 | INFO | src.logging_config | session_status | 2024 | rows=5 | cols=2\n",
      "2025-12-14 14:12:08,181 | INFO | src.logging_config | track_status | 2024 | rows=7 | cols=3\n",
      "2025-12-14 14:12:08,185 | INFO | src.logging_config | weather_data | 2024 | rows=157 | cols=8\n",
      "2025-12-14 14:12:08,196 | INFO | src.logging_config | Schema contract written to: C:\\Users\\hersh\\Desktop\\f1_analysis_project\\data\\interim\\schema_contract_columns.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cell 3 completed ‚Äî column-level schema contract generated\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 3 ‚Äî Column-level schema materialization (FIXED PATH)\n",
    "# ============================================================\n",
    "\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "from src.logging_config import setup_logging\n",
    "from src.batch_loader import load_multiple_years\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Resolve PROJECT ROOT robustly (notebooks-safe)\n",
    "# ------------------------------------------------------------\n",
    "CURRENT_DIR = Path.cwd().resolve()\n",
    "\n",
    "if CURRENT_DIR.name == \"notebooks\":\n",
    "    PROJECT_ROOT = CURRENT_DIR.parent\n",
    "else:\n",
    "    PROJECT_ROOT = CURRENT_DIR\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Setup logging\n",
    "# ------------------------------------------------------------\n",
    "logger, error_logger = setup_logging()\n",
    "logger.info(\"Cell 3 ‚Äî Column-level schema materialization started\")\n",
    "logger.info(f\"Project root resolved at: {PROJECT_ROOT}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Define paths\n",
    "# ------------------------------------------------------------\n",
    "DATA_INTERIM_DIR = PROJECT_ROOT / \"data\" / \"interim\"\n",
    "\n",
    "SCHEMA_DISCOVERY_PATH = (\n",
    "    DATA_INTERIM_DIR / \"schema_discovery_fastf1_bahrain_2022_2024.json\"\n",
    ")\n",
    "SCHEMA_CONTRACT_PATH = (\n",
    "    DATA_INTERIM_DIR / \"schema_contract_columns.json\"\n",
    ")\n",
    "\n",
    "logger.info(f\"Loading schema discovery from: {SCHEMA_DISCOVERY_PATH}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Load schema discovery metadata\n",
    "# ------------------------------------------------------------\n",
    "with open(SCHEMA_DISCOVERY_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    schema_discovery = json.load(f)\n",
    "\n",
    "logger.info(\"Schema discovery metadata loaded successfully\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Reload sessions (cached, deterministic)\n",
    "# ------------------------------------------------------------\n",
    "YEARS = sorted(int(y) for y in schema_discovery.keys())\n",
    "GP_NAME = \"Bahrain\"\n",
    "\n",
    "sessions = load_multiple_years(YEARS, GP_NAME)\n",
    "logger.info(f\"{len(sessions)} sessions loaded for schema materialization\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Prepare schema container\n",
    "# ------------------------------------------------------------\n",
    "schema_contract = defaultdict(lambda: {\n",
    "    \"columns_by_year\": {},\n",
    "    \"dtypes_by_year\": {},\n",
    "    \"row_count_by_year\": {}\n",
    "})\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 7. Materialize tables safely\n",
    "# ------------------------------------------------------------\n",
    "for session in sessions:\n",
    "    year = session.event.year\n",
    "    logger.info(f\"Materializing tables for year {year}\")\n",
    "\n",
    "    for table_name in schema_discovery[str(year)].keys():\n",
    "        try:\n",
    "            obj = getattr(session, table_name, None)\n",
    "\n",
    "            if obj is None:\n",
    "                continue\n",
    "\n",
    "            if isinstance(obj, pd.DataFrame):\n",
    "                df = obj\n",
    "            elif hasattr(obj, \"to_dataframe\"):\n",
    "                df = obj.to_dataframe()\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            schema_contract[table_name][\"columns_by_year\"][year] = list(df.columns)\n",
    "            schema_contract[table_name][\"dtypes_by_year\"][year] = {\n",
    "                col: str(dtype) for col, dtype in df.dtypes.items()\n",
    "            }\n",
    "            schema_contract[table_name][\"row_count_by_year\"][year] = len(df)\n",
    "\n",
    "            logger.info(\n",
    "                f\"{table_name} | {year} | rows={len(df)} | cols={len(df.columns)}\"\n",
    "            )\n",
    "\n",
    "        except Exception:\n",
    "            error_logger.error(\n",
    "                f\"Failed materializing '{table_name}' for year {year}\",\n",
    "                exc_info=True\n",
    "            )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 8. Persist column-level schema contract\n",
    "# ------------------------------------------------------------\n",
    "with open(SCHEMA_CONTRACT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schema_contract, f, indent=2)\n",
    "\n",
    "logger.info(f\"Schema contract written to: {SCHEMA_CONTRACT_PATH}\")\n",
    "print(\"‚úÖ Cell 3 completed ‚Äî column-level schema contract generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12dd99d4-6044-44de-9034-9719653b02a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 15:43:31,607 | INFO | src.logging_config | Cell 4 ‚Äî Schema to engineering-decision synthesis started\n",
      "2025-12-14 15:43:31,609 | INFO | src.logging_config | Project root resolved at: C:\\Users\\hersh\\Desktop\\f1_analysis_project\n",
      "2025-12-14 15:43:31,610 | INFO | src.logging_config | Loading schema contract from: C:\\Users\\hersh\\Desktop\\f1_analysis_project\\data\\interim\\schema_contract_columns.json\n",
      "2025-12-14 15:43:31,613 | INFO | src.logging_config | Schema contract loaded successfully\n",
      "2025-12-14 15:43:31,614 | INFO | src.logging_config | laps | stable=18 | normalize=12 | unsafe=1\n",
      "2025-12-14 15:43:31,615 | INFO | src.logging_config | race_control_messages | stable=8 | normalize=1 | unsafe=0\n",
      "2025-12-14 15:43:31,616 | INFO | src.logging_config | results | stable=18 | normalize=4 | unsafe=0\n",
      "2025-12-14 15:43:31,616 | INFO | src.logging_config | session_status | stable=1 | normalize=1 | unsafe=0\n",
      "2025-12-14 15:43:31,617 | INFO | src.logging_config | track_status | stable=2 | normalize=1 | unsafe=0\n",
      "2025-12-14 15:43:31,618 | INFO | src.logging_config | weather_data | stable=7 | normalize=1 | unsafe=0\n",
      "2025-12-14 15:43:31,621 | INFO | src.logging_config | Engineering decisions written to: C:\\Users\\hersh\\Desktop\\f1_analysis_project\\data\\interim\\schema_engineering_decisions.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cell 4 completed ‚Äî schema translated into engineering decisions\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Cell 4 ‚Äî Schema ‚Üí Engineering Decision Synthesis (Final)\n",
    "# ============================================================\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "from src.logging_config import setup_logging\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. Setup logging\n",
    "# ------------------------------------------------------------\n",
    "logger, error_logger = setup_logging()\n",
    "logger.info(\"Cell 4 ‚Äî Schema to engineering-decision synthesis started\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. Resolve project root robustly\n",
    "# ------------------------------------------------------------\n",
    "def resolve_project_root(start: Path) -> Path:\n",
    "    for parent in [start] + list(start.parents):\n",
    "        if (parent / \"src\").exists() and (parent / \"data\").exists():\n",
    "            return parent\n",
    "    raise RuntimeError(\"Project root could not be resolved\")\n",
    "\n",
    "PROJECT_ROOT = resolve_project_root(Path.cwd())\n",
    "DATA_INTERIM_DIR = PROJECT_ROOT / \"data\" / \"interim\"\n",
    "\n",
    "SCHEMA_CONTRACT_PATH = DATA_INTERIM_DIR / \"schema_contract_columns.json\"\n",
    "DECISION_OUTPUT_PATH = DATA_INTERIM_DIR / \"schema_engineering_decisions.json\"\n",
    "\n",
    "logger.info(f\"Project root resolved at: {PROJECT_ROOT}\")\n",
    "logger.info(f\"Loading schema contract from: {SCHEMA_CONTRACT_PATH}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. Load schema contract\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    with open(SCHEMA_CONTRACT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        schema_contract = json.load(f)\n",
    "except Exception:\n",
    "    error_logger.error(\"Failed to load schema contract\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "logger.info(\"Schema contract loaded successfully\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4. Decision rules\n",
    "# ------------------------------------------------------------\n",
    "TIME_DTYPES = {\"timedelta64[ns]\", \"datetime64[ns]\"}\n",
    "NUMERIC_DTYPES = {\"int64\", \"float64\", \"bool\"}\n",
    "OBJECT_DTYPE = \"object\"\n",
    "\n",
    "engineering_decisions = {}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5. Synthesize engineering decisions (schema-level only)\n",
    "# ------------------------------------------------------------\n",
    "for table_name, meta in schema_contract.items():\n",
    "    dtype_tracker = defaultdict(set)\n",
    "\n",
    "    dtypes_by_year = meta[\"dtypes_by_year\"]\n",
    "\n",
    "    for year, dtype_map in dtypes_by_year.items():\n",
    "        for col, dtype in dtype_map.items():\n",
    "            dtype_tracker[col].add(dtype)\n",
    "\n",
    "    stable_columns = []\n",
    "    requires_normalization = []\n",
    "    unsafe_or_contextual = []\n",
    "\n",
    "    for col, dtypes in dtype_tracker.items():\n",
    "        # dtype changes across years ‚Üí unsafe\n",
    "        if len(dtypes) > 1:\n",
    "            unsafe_or_contextual.append(col)\n",
    "            continue\n",
    "\n",
    "        dtype = next(iter(dtypes))\n",
    "\n",
    "        if dtype in TIME_DTYPES:\n",
    "            requires_normalization.append(col)\n",
    "        elif dtype == OBJECT_DTYPE or dtype in NUMERIC_DTYPES:\n",
    "            stable_columns.append(col)\n",
    "        else:\n",
    "            unsafe_or_contextual.append(col)\n",
    "\n",
    "    engineering_decisions[table_name] = {\n",
    "        \"stable_columns\": sorted(stable_columns),\n",
    "        \"requires_normalization\": sorted(requires_normalization),\n",
    "        \"unsafe_or_contextual\": sorted(unsafe_or_contextual),\n",
    "        \"notes\": {\n",
    "            \"stable_columns\": \"Stable across seasons; safe for ingestion and modeling\",\n",
    "            \"requires_normalization\": \"Time or semantic fields requiring normalization\",\n",
    "            \"unsafe_or_contextual\": \"Detected type drift or API volatility\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    logger.info(\n",
    "        f\"{table_name} | stable={len(stable_columns)} | \"\n",
    "        f\"normalize={len(requires_normalization)} | \"\n",
    "        f\"unsafe={len(unsafe_or_contextual)}\"\n",
    "    )\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6. Persist engineering decisions\n",
    "# ------------------------------------------------------------\n",
    "try:\n",
    "    with open(DECISION_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(engineering_decisions, f, indent=2)\n",
    "except Exception:\n",
    "    error_logger.error(\"Failed to write engineering decision artifact\", exc_info=True)\n",
    "    raise\n",
    "\n",
    "logger.info(f\"Engineering decisions written to: {DECISION_OUTPUT_PATH}\")\n",
    "print(\"‚úÖ Cell 4 completed ‚Äî schema translated into engineering decisions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05491650-d560-4ac8-890d-b3ea5c3981cc",
   "metadata": {},
   "source": [
    "# ‚úÖ Notebook 00 ‚Äî Conclusion & Findings\n",
    "\n",
    "## üèÅ What We Accomplished\n",
    "\n",
    "This notebook **successfully and exhaustively completed all schema discovery objectives** defined in the project pipeline and the Specification_F1 document.\n",
    "\n",
    "By the end of this notebook, we transitioned from *zero assumptions* about the FastF1 API to a **fully validated, engineering-ready schema contract** that downstream notebooks can rely on without risk.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Environment & Infrastructure Setup\n",
    "\n",
    "We resolved several foundational execution and reproducibility issues:\n",
    "\n",
    "- Ensured **all file paths are project-root anchored**, never notebook-relative\n",
    "- Configured centralized logging with:\n",
    "  - `project.log` ‚Üí execution flow, decisions, progress\n",
    "  - `errors.log` ‚Üí stack traces and failure diagnostics\n",
    "- Initialized FastF1 with:\n",
    "  - Persistent on-disk caching\n",
    "  - Safe reuse of cached data across runs\n",
    "\n",
    "üß† **Key insight:**  \n",
    "Notebook execution context must never dictate where data, logs, or artifacts are written.  \n",
    "All pipeline stages must behave identically regardless of where or how the notebook is run.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Object-Level Schema Discovery (Zero Assumptions)\n",
    "\n",
    "We empirically inspected FastF1 session objects for:\n",
    "\n",
    "- üèÅ Bahrain Grand Prix (Race session)\n",
    "- üìÖ Seasons: **2022, 2023, 2024**\n",
    "\n",
    "Instead of assuming any schema, we **loaded real sessions** and introspected them directly.\n",
    "\n",
    "### What we discovered\n",
    "\n",
    "- Each session exposes **multiple schema-bearing objects**\n",
    "- These objects consistently appear across all tested seasons\n",
    "- Each object maps cleanly to a **logical table-like dataset**\n",
    "\n",
    "Examples include:\n",
    "- `laps`\n",
    "- `results`\n",
    "- `weather_data`\n",
    "- `race_control_messages`\n",
    "- `track_status`\n",
    "- `session_status`\n",
    "- car- and position-related telemetry tables\n",
    "\n",
    "üìÑ **Artifact produced:**  \n",
    "`schema_discovery_fastf1_bahrain_2022_2024.json`\n",
    "\n",
    "üìå **What this file tells us:**\n",
    "- Exactly which objects behave like tables\n",
    "- Which objects are present across seasons\n",
    "- Which datasets are viable pipeline inputs\n",
    "\n",
    "This eliminated **all uncertainty** about what data FastF1 actually provides.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Column-Level Schema Materialization\n",
    "\n",
    "Next, we materialized every discovered table and inspected its **real, runtime structure**.\n",
    "\n",
    "For **every table in every year**, we extracted:\n",
    "\n",
    "- Column names\n",
    "- Pandas data types\n",
    "- Row counts\n",
    "\n",
    "This step replaced abstract API documentation with **empirical truth**.\n",
    "\n",
    "üìÑ **Artifact produced:**  \n",
    "`schema_contract_columns.json`\n",
    "\n",
    "üìå **What this file tells us:**\n",
    "- Exact column names (no guessing)\n",
    "- Exact data types encountered in reality\n",
    "- Whether schemas are stable or drifting across seasons\n",
    "- Which tables represent:\n",
    "  - Fact-level data (laps, results)\n",
    "  - Event-level data (race control messages)\n",
    "  - Metadata/state data (session_status, track_status, weather)\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Schema ‚Üí Engineering Decision Synthesis\n",
    "\n",
    "We then translated raw schema facts into **explicit engineering decisions**.\n",
    "\n",
    "This step answers the most important pipeline question:\n",
    "\n",
    "> *How should each column be treated downstream?*\n",
    "\n",
    "üìÑ **Artifact produced:**  \n",
    "`schema_engineering_decisions.json`\n",
    "\n",
    "### üîé Concrete conclusions derived\n",
    "\n",
    "#### ‚úÖ Stable Columns (Safe for Direct Ingestion)\n",
    "Columns that:\n",
    "- Exist in all years\n",
    "- Maintain consistent data types\n",
    "- Can be loaded without transformation risk\n",
    "\n",
    "Examples:\n",
    "- `results.Position`, `results.DriverNumber`\n",
    "- `laps.LapNumber`\n",
    "- `weather_data.AirTemp`, `weather_data.TrackTemp`\n",
    "\n",
    "These columns can be **trusted as-is**.\n",
    "\n",
    "---\n",
    "\n",
    "#### üîÑ Columns Requiring Normalization\n",
    "Columns that:\n",
    "- Represent time, duration, or session-relative values\n",
    "- Require conversion to consistent units or formats\n",
    "- Are unsuitable for raw analytical queries\n",
    "\n",
    "Examples include:\n",
    "- Time-based lap columns (`LapTime`, `SectorTime`)\n",
    "- Session-relative timestamps\n",
    "- Weather or telemetry fields requiring unit alignment\n",
    "\n",
    "These columns **must be normalized in later notebooks**, not here.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö†Ô∏è Unsafe / Contextual Columns\n",
    "Columns that:\n",
    "- Change semantics across seasons\n",
    "- Depend on session-specific interpretation\n",
    "- Are not analytically stable without additional logic\n",
    "\n",
    "Examples:\n",
    "- Flags such as `IsPersonalBest`\n",
    "- Contextual status indicators\n",
    "\n",
    "These columns must be:\n",
    "- Either excluded\n",
    "- Or handled with explicit business logic later\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Errors & Problems Encountered (and Solved)\n",
    "\n",
    "We encountered and resolved several critical issues during execution:\n",
    "\n",
    "| Problem | Resolution |\n",
    "|------|-----------|\n",
    "Assumed column names | Eliminated via runtime inspection |\n",
    "Notebook-relative paths | Fixed via project-root anchoring |\n",
    "Lazy-loaded FastF1 objects | Resolved through explicit materialization |\n",
    "Schema mismatches | Corrected by aligning logic with real JSON artifacts |\n",
    "Missing dtype assumptions | Replaced with empirical dtype extraction |\n",
    "\n",
    "Each failure exposed an incorrect assumption and resulted in a **more robust pipeline design**.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ What We Now Know with Certainty\n",
    "\n",
    "At the end of Notebook 00:\n",
    "\n",
    "- ‚úÖ Every table exposed by FastF1 is known\n",
    "- ‚úÖ Every column name is known\n",
    "- ‚úÖ Every column‚Äôs runtime data type is known\n",
    "- ‚úÖ Schema stability across **2022‚Äì2024** is verified\n",
    "- ‚úÖ Columns needing normalization are explicitly identified\n",
    "- ‚úÖ Unsafe or contextual columns are explicitly flagged\n",
    "\n",
    "This represents the **maximum possible certainty** before transformation work begins.\n",
    "\n",
    "---\n",
    "\n",
    "## üö´ What This Notebook Intentionally Did NOT Do\n",
    "\n",
    "By design, this notebook **did not** perform:\n",
    "\n",
    "- Data cleaning\n",
    "- Normalization\n",
    "- Business logic\n",
    "- Database insertion\n",
    "- Analytics or aggregations\n",
    "\n",
    "Performing those steps here would violate the pipeline‚Äôs separation of concerns.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚û°Ô∏è Next Steps ‚Äî Notebook 01 (as defined in the Pipeline Document)\n",
    "\n",
    "### üìò Notebook 01: Multi-Year Raw Data Ingestion\n",
    "\n",
    "In the next notebook, we will:\n",
    "\n",
    "- Load multi-year Formula 1 data at scale\n",
    "- Rely strictly on the schema contracts defined here\n",
    "- Persist raw datasets deterministically\n",
    "- Prepare data for normalization and database modeling\n",
    "\n",
    "All downstream notebooks will **treat the outputs of Notebook 00 as authoritative**.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Final Statement\n",
    "\n",
    "Notebook 00 has fully achieved its purpose.\n",
    "\n",
    "It establishes a **schema-verified, assumption-free foundation** for the entire Formula One end-to-end analytics pipeline.\n",
    "\n",
    "With uncertainty eliminated at the schema level, all future notebooks can focus exclusively on **transformation, modeling, and analysis** ‚Äî not defensive debugging.\n",
    "\n",
    "This notebook is now **closed by design**. üèÅ\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
